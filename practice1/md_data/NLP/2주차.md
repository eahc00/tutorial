## Representation learning(표현 학습)
- Representation learning이 무엇인가? 이게 과거와 지금 그리고 앞으로의 패러다임이 바뀌고 있다.
- Representation learning을 잘하려고 하다보면 딥러닝이라는 도구가 도출이 되더라.
- 데이터를 잘 다루려고 하다보면 결국은 다시 표현학습 혹은 딥러닝하고 연결이 되더라.


### 표현 학습이 무엇인가?

사물 인지 → 사전적 정의(말) 기반 : 예외가 나오면 못 함.
만약 사다리 사진의 일부를 가리고 복원하라고 수행하며 사다리 사진 1억장을 준다 → 기계가 이 모든 사진을 복원할 수 있다면 기계가 사다리를 알고 있는 것 아닐까…?
→ 자연어에도 똑같이 대량의 문장을 주며 문장의 일부를 가리고 복원하라고 하면?
  
> [!definition]
> **표현학습 : 사물의 특징을 스스로 파악하도록 학습하는 것.**
  
사과를 표현
인간의 방식 : 당도, 색, 모양… (property : value) → DataFrame으로 표현
→ 표현학습에서는? : **숫자들의 덩어리로 표현(Numbers) ; 기계가 이해하기 편한 형태.**
  
- **Distributed Representation(분산 표현)**
	- DNN가 기존 AI 방법론들에 비해 큰 의미가 있는 것은 실세계에 있는 실제 Object를 표현할 때 **Symbol에 의존하지 않는다**는 것.
	- One-Hot Representation(기호와 1대1 매핑, sparse) : 크기가 단어 수(vocab_size)가 된다.
	- Distributed Representation(**분산 표현 방식, dense**) : 정보가 모든 곳에 퍼져있음. 단어를 표현하는 벡터.


### 표현학습의 패러다임
- **Cognitive Science**(인지과학) 
	- 인간-인간, 인간-동물, 인간-인공물 간의 **정보처리** 활동을 다룸. 
	- 사람이나 동물의 (머리 속의) **프로세스가 작동되는 방식 자체**를 연구.
- **Artificial Intelligence** 
	- 지능을 다룸(사람의 지능 → 사람 + @).
	- 인공적으로 지능을 만들어가는 시도.
- **Machine Learning** 
	- 기계가 스스로 **학습하도록 하는 학습 능력**을 다룸. 
	- 지능 개발이 목표. 
	- Application 개발과 연관.
- **Deep Learning** 
	- NN을 쓰는 머신러닝을 구현하려고 하는 모든 시도
- **Data Mining**(Data Science) 
	- 데이터 안에서 패턴을 찾아보거나, 그룹 표현을 해주거나 인사이트를 얻는다. **데이터 분석**이 목표.
  
Cognitive Science > Artificial Intelligence > Machine Learning > Deep Learning ←—기술 공유—→ Data Mining
  

### 표현학습: 딥러닝, 데이터
### Latent Variable 잠재 변수
> [!summary]
> - Deep Neural Network의 핵심
> - Essence of Modern Machine Learning
> - Hidden Variable
> → 현상과 사물의 특징을 **기계가 스스로** 파악
> 

- DNN의 핵심, 현대 기계학습의 근본, **Hidden Variable**
	- x : 실세계에 존재하는 **관측 가능**한 것.
	    관측 가능 → Count 가능 → P(x) : 확률을 구할 수 있다.(귀납적으로)
    
	- h : 이 세상에 존재하지 않는 **가상의 값**. 간접적으로 추측만 가능. 무엇이든 될 수 있는 값. 우리는 이 h를 배워내는 것이 목표.
  
→ 의미상 h는 **무한의 semantic space**를 가지고 있음
→ h의 의미 영역을 줄이기 위해 **x - h를 (수학적으로) 묶어줌**. x와 연관된 h로 한정(x가 움직일 때 h도 움직이게끔). 의미 영역이 줄어든다. but, 여전히 h는 x의 원인이든, x의 결과이든 비슷하게 움직이는 값이든 어떤 값도 될 수 있다.
→ x의 데이터가 엄청 많다면? : x의 변화 → h의 변화를 관측하여 더 좁힐 수 있다. but, 여전히 h는 어떤 값도 될 수 있다.
→ 다른 실세계에서 관측 가능한 y도 같이 연관시키면? h가 더 줄어듬.(더 많은 변수도 연관 시킬 수 있음)
이런 식으로 **디자인**을 잘하게 되면 h의 값을 내가 원하는 범위로 setting할 수 있다.
  
⇒ **Latent Variable의 의미영역**을 축소시킬 수 있는 도구
1. 많은 수의 **데이터**(빅데이터, 클라우드 시대…)
2. **구조적 연관성**(컴퓨터 연산능력… 증가)
→ 데이터를 관통하는 핵심적인 h를 얻을 수 있다.

구조적 연관성이 **복잡할수록**(= 엣지 수가 많아질 수록, = 계산의 연산량이 많아질수록) 잘 된다. 


#### Example
- 과일 인식기
	- 9픽셀의 사과 이미지 → “사과” : 9차원 ——(어떤 과정?)——> 1차원 라벨
	- 이때 중간에서는 계속 추상화 하는 과정을 하지 않을까? (이게 내가 찾고 싶은 h가 된다.) : 사과를 사과라고 할 수 있는 뭔가를 배워내는 것.
	- x —— h ——> y (고차원 → 저차원)
	- **h : x를 설명하고 y를 발생시킨다. (Describe x and Cause y)**
	- h layer를 여러 개 쌓으면 추정해야 하는 h의 수가 x, y수보다 훨씬 많아진다. 데이터가 이만큼 많아야 이 모든 hidden variable 혹은 latent variable이 학습이 되게 된다.


표현학습의 예 : Auto Encoder, 위에서 설명한 분류 방식
(더 나가면 diffusion, self-supervised learning 등이 있다.)


### Representation이 우리 사회에 끼치는 영향?
- 기존의 Process
    - 사물 → **특징 추출** : 사람이 만든 **규칙**에 의한 → 특징 → (AI)Algorithm
    - 특징 : 사람이 만든 것 → 복제가 가능하다. (사람을 데려오거나 특허를 사거나)
	
- 새로운 Process
    - 사물 → **특징 연산** : **학습된 파라미터**에 의한 → 숫자 → (AI)Algorithm
    - 특징 : 숫자를 만들어내는 과정(convert)이 복제가 안된다.
    - 이 Converter가 표현 학습의 결과물이다.
	- 사물, 개념 → Number


## Data 관점
- **고차원 → 저차원의 문제**
- V to 1
    ex) 3x3 data를 하나로 요약하는 방법?
    → center, median, average
    +Weigth의 개념 추가 : Weighted Sum, Weighted Average
    → CNN의 개념. Fully Connected Network(parameter sharing)
	
    ex2) 3x3 data를 두 개로 요약하는 방법은?
    → weight판을 두 개 만들면 된다.
    ```Python
    nn.linear(9, 2) # torch
    Dense(9, 2) # tensorflow
    ```
    
  
- 부가 정보가 필요할 때 어떻게 반영?
    부가정보를 위한 weight판을 또 만들어서 더해주면 된다.
    → Concatenate 시키면 된다.
    
- V → V’ → V’’→ … → 1 : MLP
    ⇒ 결국 y = ax + b와 같음.



# Sequence to Sequence Learning
Machine Learning의 Domain이 넓어짐. (Seq2seq기술)

> [!definition]
>- 입력에 대한 Sequence ——(관계 학습)——> 출력에 대한 Sequence
>    - 이 중간에 **관계를 학습하는 모든 방법**을 Seq2Seq라고 부른다.


- 입력 차원 N, 출력 차원 M
    - N = M
    - N ≠ M, M = 1 → Classification
    - N ≠ M, M ≥ 1(입, 출력이 동적)

- S2S example
    - Speech Recognition(음성인식) : 입력 음성을 window단위로 뭐가 나오는지 관계 학습.
        openAI whisper : S2S 방식 음성인식. pairwise data…?
        
    - Movie Frame Labeling
        Input : Video Frame → Output :Scene Label
        프레임마다 이벤트에 대한 라벨링을 붙여줌.
        
    - **POS Tagging**
        Input : Text → Output : 품사 etc…
        
    - Arithmetic Calculation
        Input : Math Expression → Output : Numbers
        
    - Machine Translation
        Input : Korean Text → Output : English Text
        
    - Sentence Completion
        Input : Partial Sentence → Output : Partial Sentence
        
    


### S2S DEEP LEARNING APPROACH
- 여러 번(독립적으로) Prediction(Sequence 길이만큼) + Convolution Idea
- **Sequence를 모델링**(**순서**를 넣어보자!)
    - 입력 순서를 달아 준다. **기억과 연관**된다.(네트워크가 기억력을 가짐)
        기억 : 과거의 어떤 것이 현재에 영향을 미치는 것.
    - **Neural Network + Memory = Recurrent Neural Network**
        - 결과물이 다음 번의 입력으로 다시 들어옴.


- RNN의 문제(한계)
    - **Vanishing Gradient**
        - (backpropagation시)term이 길어질 때 Gradient가 소실된다.
        - 구조적으로 바로 전의 것에 영향을 받게 되어 있기 때문에 Long-Dependency가 있는 입력 혹은 결과가 잘 반영되지 않는다.
	- **Exploding Gradient**
		- 더 중요한 정보, 덜 중요한 정보를 구분하지 못하고 Gradient가 폭발할 수 있다.
	    - Short-Dependency가 더 중요함에도, 멀리 있는 gradient가 너무 높게 계산 되어 가까이 있는 gradient가 충분히 반영되지 않는 경우.


- 기존 RNN은 sequence가 길어지면 성능이 떨어짐.
	→ LSTM이 이걸 해결하는 방법으로 나옴.
	→ LSTM RNN으로 최초의 S2S을 개발.

- LSTM(개념만 알아두면 됨.)

| neural | memory | 의미                                                     |
| ------ | ------ | ------------------------------------------------------ |
| input  | Write  | 1이면 입력 x가 들어올 수 있도록 허용(open). 0이면 Block(closed)        |
| output | Read   | 1이면 의미있는 결과물로 최종 Output(open). 0이면 해당 연산 출력 안함(closed) |
| forget | Reset  | 1이면 바로 전 time의 memory를 유지. 0이면 reset. Keep gate        |
	- 각 Gate에 현재와 과거의 정보가 모두 반영되도록 Network 설계

- GRU
	- Update Gate를 두어서 현재 Time의 Hidden state를 계산할 때 update gate의 영향을 받도록 함.
	- Reset Gate를 두어서 현재 Memory를 Reset할지 안할지를 결정

→ attention, transformer

- Sequence Modeling for POS Tagging : NtoN
- S2S example - Programming Code Generation(N21)



### Encoding-Decoding Approach
- Encoding Approach
    - 전달된 각 시퀀스 전부가 아닌 **그걸 가지고 나온 하나의 숫자**를 얻고싶다. : 가장 **마지막 RNN node의 Hidden Variable.**
    - Output이 잘 나오도록 parameter 조정됨, 즉, 최종 Output이 잘 나오도록 하는 정보를 RNN Layer에 기억하게 됨.
    - Idea : RNN에 누적된 정보가 결국 Sequence의 Vector Form일 것이다.
- Decoding Approach
    - 가장 마지막 RNN node의 Hidden Approach를 이용(복사)하여 RNN을 또 만든다. → Output이 나온다.


- Translation Pyramid
    - interlingua : 세상 모든 언어를 관통하는 가상의 언어
    - source text ——(analysis)——> interlingua ——(generation)——> target text
    - 이 interlingua가 중간 hidden vector가 된다.


#### Attention Modeling
- Bidirectional RNN for Encoding
- Performance - Attention Modeling @ Machine Translation
	- 선별적으로 가중치가 적용된 Encoding이 적용됨으로서, 긴 문장에서도 번역 성능이 덜어지지 않는다.
