{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Korpora] Corpus `naver_changwon_ner` is already installed at /home/eahc00/Korpora/naver_changwon_ner/train_data\n"
     ]
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "Korpora.fetch('naver_changwon_ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : 네이버 + 창원대\n",
      "    Repository : https://github.com/naver/nlp-challenge/tree/master/missions/ner\n",
      "    References : http://air.changwon.ac.kr/?page_id=10\n",
      "\n",
      "    개체명(Named Entity)은 인명, 기관명, 지명 등과 같이 문장 또는 문서에서 특정한 의미를 가지고 있는 단어 또는 어구를 말합니다.\n",
      "    이 때문에 개체명은 정보 검색 및 언어 이해를 위한 분석에서 주요한 대상으로 다루어지고 있습니다.\n",
      "    Data.ly에서는 개체명 코퍼스를 제공하여 연구에 도움을 드리고자 하며, 공개적인 리더보드를 통해 많은 분들의 연구 동향을 논의/공유하고자 합니다.\n",
      "    제공되는 코퍼스는 Data.ly에서 제작한 것으로, 연구 및 리더보드를 위한 학습으로 사용 가능하며 상업적인 목적으로 사용될 수 없습니다.\n",
      "\n",
      "    # License\n",
      "    연구 및 리더보드를 위한 학습으로 사용 가능하며 상업적인 목적으로 사용될 수 없습니다.\n",
      "\n",
      "[Korpora] Corpus `naver_changwon_ner` is already installed at /home/eahc00/tutorial/practice1/Korpora/naver_changwon_ner/train_data\n"
     ]
    }
   ],
   "source": [
    "corpus = Korpora.load('naver_changwon_ner', root_dir='/home/eahc00/tutorial/practice1/Korpora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordTag(text='비토리오 양일 만에 영사관 감호 용퇴, 항룡 압력설 의심만 가율 ', words=['비토리오', '양일', '만에', '영사관', '감호', '용퇴,', '항룡', '압력설', '의심만', '가율'], tags=['PER_B', 'DAT_B', '-', 'ORG_B', 'CVL_B', '-', '-', '-', '-', '-'])\n",
      "90000\n",
      "비토리오 양일 만에 영사관 감호 용퇴, 항룡 압력설 의심만 가율 \n",
      "['PER_B', 'DAT_B', '-', 'ORG_B', 'CVL_B', '-', '-', '-', '-', '-']\n",
      "['비토리오', '양일', '만에', '영사관', '감호', '용퇴,', '항룡', '압력설', '의심만', '가율']\n"
     ]
    }
   ],
   "source": [
    "print(corpus.train[0])\n",
    "\n",
    "print(len(corpus.train))\n",
    "\n",
    "print(corpus.get_all_texts()[0])\n",
    "print(corpus.get_all_tags()[0])\n",
    "print(corpus.get_all_words()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordTag(text='9세이브로 구완 30위인 LG 박찬형은 평균자책점이 16.45로 준수한 편이지만 22⅓이닝 동안 피홈런이 31개나 된다 . ', words=['9세이브로', '구완', '30위인', 'LG', '박찬형은', '평균자책점이', '16.45로', '준수한', '편이지만', '22⅓이닝', '동안', '피홈런이', '31개나', '된다', '.'], tags=['NUM_B', '-', 'NUM_B', 'ORG_B', 'PER_B', '-', 'NUM_B', '-', '-', 'NUM_B', '-', '-', 'NUM_B', '-', '-'])\n"
     ]
    }
   ],
   "source": [
    "print(corpus.train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaverChangwonNER.train: size=90000\n",
       "  - NaverChangwonNER.train.texts : list[str]\n",
       "  - NaverChangwonNER.train.words : list[list]\n",
       "  - NaverChangwonNER.train.tags : list[list]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = corpus.get_all_tags()\n",
    "\n",
    "tag_set = set()\n",
    "\n",
    "for tag in tags:\n",
    "    tag = set(tag)\n",
    "    tag_set.update(tag)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "['-', 'AFW_B', 'AFW_I', 'ANM_B', 'ANM_I', 'CVL_B', 'CVL_I', 'DAT_B', 'DAT_I', 'EVT_B', 'EVT_I', 'FLD_B', 'FLD_I', 'LOC_B', 'LOC_I', 'MAT_B', 'MAT_I', 'NUM_B', 'NUM_I', 'ORG_B', 'ORG_I', 'PER_B', 'PER_I', 'PLT_B', 'PLT_I', 'TIM_B', 'TIM_I', 'TRM_B', 'TRM_I']\n"
     ]
    }
   ],
   "source": [
    "print(len(tag_set))\n",
    "print(sorted(list(tag_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331196\n"
     ]
    }
   ],
   "source": [
    "words = corpus.get_all_words()\n",
    "\n",
    "word_set = set()\n",
    "\n",
    "for word in words:\n",
    "    word = set(word)\n",
    "    word_set.update(word)\n",
    "\n",
    "print(len(word_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eahc00/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_model\n",
    "from tutorial.practice1.model import Model\n",
    "from tutorial.commons import BERT_CONFIG\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "hg_config = BertConfig.from_pretrained(model_name)\n",
    "num_lables = 29\n",
    "\n",
    "my_config = BERT_CONFIG(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    padding_idx=tokenizer.convert_tokens_to_ids(\"[PAD]\"),\n",
    "    max_seq_length=hg_config.max_position_embeddings,\n",
    "    d_model=hg_config.hidden_size,\n",
    "    layer_norm_eps=hg_config.layer_norm_eps,\n",
    "    emb_hidden_dropout=hg_config.hidden_dropout_prob,\n",
    "    num_layers=hg_config.num_hidden_layers,\n",
    "    num_heads=hg_config.num_attention_heads,\n",
    "    att_prob_dropout=hg_config.attention_probs_dropout_prob,\n",
    "    dim_feedforward=hg_config.intermediate_size,\n",
    "    pooled_output=False,\n",
    ")\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    model_name=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    bert_config=my_config,\n",
    "    num_labels=num_lables,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['비', '##토', '##리', '##오'], 'PER_B')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tokenizer.tokenize(corpus.train[0].words[0])\n",
    "b = corpus.train[0].tags[0]\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_model\n",
    "\n",
    "trained_model = load_model(model, \"../ner_model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = \"관세청은 울산 경남 경북지역 등에 대규모 산불이 발생함에 따라 신속한 복구와 피해기업 지원을 위한 관세행정 종합지원방안을 수립해 추진한다고 26일 밝혔다.\"\n",
    "\n",
    "words = input_sentence.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : 네이버 + 창원대\n",
      "    Repository : https://github.com/naver/nlp-challenge/tree/master/missions/ner\n",
      "    References : http://air.changwon.ac.kr/?page_id=10\n",
      "\n",
      "    개체명(Named Entity)은 인명, 기관명, 지명 등과 같이 문장 또는 문서에서 특정한 의미를 가지고 있는 단어 또는 어구를 말합니다.\n",
      "    이 때문에 개체명은 정보 검색 및 언어 이해를 위한 분석에서 주요한 대상으로 다루어지고 있습니다.\n",
      "    Data.ly에서는 개체명 코퍼스를 제공하여 연구에 도움을 드리고자 하며, 공개적인 리더보드를 통해 많은 분들의 연구 동향을 논의/공유하고자 합니다.\n",
      "    제공되는 코퍼스는 Data.ly에서 제작한 것으로, 연구 및 리더보드를 위한 학습으로 사용 가능하며 상업적인 목적으로 사용될 수 없습니다.\n",
      "\n",
      "    # License\n",
      "    연구 및 리더보드를 위한 학습으로 사용 가능하며 상업적인 목적으로 사용될 수 없습니다.\n",
      "\n",
      "[Korpora] Corpus `naver_changwon_ner` is already installed at /home/eahc00/tutorial/practice1/Korpora/naver_changwon_ner/train_data\n",
      "Words: ['관세청은', '울산', '경남', '경북지역', '등에', '대규모', '산불이', '발생함에', '따라', '신속한', '복구와', '피해기업', '지원을', '위한', '관세행정', '종합지원방안을', '수립해', '추진한다고', '26일', '밝혔다.']\n",
      "Predicted NER tags: ['PLT_B', 'TRM_I', 'TRM_I', 'TRM_I', 'PLT_B', 'PLT_B', 'PER_I', 'ANM_I', 'ANM_I', 'TIM_I', 'EVT_I', 'DAT_B', 'CVL_B', 'ANM_I', 'LOC_B', 'LOC_B', 'PLT_B', 'ANM_I', 'DAT_I', '-']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "import torch\n",
    "from tutorial.practice1.dataset import NERDataModule\n",
    "\n",
    "\n",
    "# 1. 모델과 토크나이저 불러오기\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "load_model(model, \"../ner_model.safetensors\")\n",
    "dataloader = NERDataModule(tokenizer, 8, 512)\n",
    "dataloader.setup()\n",
    "\n",
    "# 2. 입력 문장 전처리\n",
    "input_sentence = \"관세청은 울산 경남 경북지역 등에 대규모 산불이 발생함에 따라 신속한 복구와 피해기업 지원을 위한 관세행정 종합지원방안을 수립해 추진한다고 26일 밝혔다.\"\n",
    "words = input_sentence.split()\n",
    "\n",
    "encoded = tokenizer(\n",
    "    words,\n",
    "    is_split_into_words=True,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "input_ids = encoded.input_ids\n",
    "attention_mask = encoded.attention_mask\n",
    "\n",
    "# 3. 모델 추론\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask)\n",
    "    logits = outputs[\"logits\"]\n",
    "\n",
    "# 4. 토큰별 예측 결과\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# 5. 원래 단어 단위로 매핑\n",
    "word_ids = encoded.word_ids(batch_index=0)\n",
    "pred_labels = predictions[0].tolist()\n",
    "\n",
    "word_predictions = []\n",
    "previous_word_idx = None\n",
    "for idx, word_idx in enumerate(word_ids):\n",
    "    if word_idx is None:\n",
    "        continue\n",
    "    if word_idx != previous_word_idx:\n",
    "        word_predictions.append(pred_labels[idx])\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "# 6. 인덱스를 실제 태그로 변환 (학습 시 사용한 label2idx에 맞춰서 수정)\n",
    "idx2label = {idx: label for label, idx in dataloader.label2idx.items()}\n",
    "predicted_tags = [idx2label[label_idx] for label_idx in word_predictions]\n",
    "\n",
    "print(\"Words:\", words)\n",
    "print(\"Predicted NER tags:\", predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
