{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "client = MilvusClient(\"milvus_demo.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if client.has_collection(collection_name=\"demo_collection\"):\n",
    "    client.drop_collection(collection_name=\"demo_collection\")\n",
    "client.create_collection(\n",
    "    collection_name=\"demo_collection\",\n",
    "    dimension=1024,  # The vectors we will use in this demo has 768 dimensions\n",
    "    vector_field_name=\"dense\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markdown 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/eahc00/tutorial/practice1/md_data/NLP/2주차.md\", \"r\") as f:\n",
    "    docs = f.read()\n",
    "    # print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### markdown chuncking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [  # 문서를 분할할 헤더 레벨과 해당 레벨의 이름을 정의합니다.\n",
    "    (\n",
    "        \"#\",\n",
    "        \"Header 1\",\n",
    "    ),  # 헤더 레벨 1은 '#'로 표시되며, 'Header 1'이라는 이름을 가집니다.\n",
    "    (\n",
    "        \"##\",\n",
    "        \"Header 2\",\n",
    "    ),  # 헤더 레벨 2는 '##'로 표시되며, 'Header 2'라는 이름을 가집니다.\n",
    "    (\n",
    "        \"###\",\n",
    "        \"Header 3\",\n",
    "    ),  # 헤더 레벨 3은 '###'로 표시되며, 'Header 3'이라는 이름을 가집니다.\n",
    "    (\n",
    "        \"####\",\n",
    "        \"Header 4\",\n",
    "    )\n",
    "]\n",
    "\n",
    "# 마크다운 헤더를 기준으로 텍스트를 분할하는 MarkdownHeaderTextSplitter 객체를 생성합니다.\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False,)\n",
    "# markdown_document를 헤더를 기준으로 분할하여 md_header_splits에 저장합니다.\n",
    "md_header_splits = markdown_splitter.split_text(docs)\n",
    "docs = []\n",
    "# 분할된 결과를 출력합니다.\n",
    "for header in md_header_splits:\n",
    "    docs.append(header.page_content)\n",
    "    # print(f\"{header.page_content}\")\n",
    "    # print(f\"{header.metadata}\", end=\"\\n=====================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding(dense, sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eahc00/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 30 files: 100%|██████████| 30/30 [00:00<00:00, 61410.01it/s]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: {'dense': [array([-0.03967127, -0.00750555, -0.02115396, ..., -0.05537705,\n",
      "        0.07631838, -0.0071889 ], shape=(1024,), dtype=float32), array([-0.03342912, -0.01282029, -0.0105438 , ..., -0.03197164,\n",
      "        0.03826621,  0.02119539], shape=(1024,), dtype=float32), array([-0.03467105, -0.03611848,  0.00391398, ..., -0.01570554,\n",
      "        0.07241791, -0.00499846], shape=(1024,), dtype=float32), array([-0.00449571, -0.01025842, -0.02835798, ...,  0.00889839,\n",
      "        0.04279955,  0.00064747], shape=(1024,), dtype=float32), array([-0.05673527, -0.02733854, -0.01462478, ..., -0.01317806,\n",
      "        0.00023753, -0.003809  ], shape=(1024,), dtype=float32), array([-0.06701937, -0.02986659,  0.01342021, ..., -0.00878381,\n",
      "        0.00974623,  0.06811511], shape=(1024,), dtype=float32), array([-0.05286963, -0.00155859, -0.01700661, ..., -0.02146157,\n",
      "        0.002679  , -0.00604413], shape=(1024,), dtype=float32), array([-0.0120805 , -0.02733046, -0.03279171, ...,  0.00950356,\n",
      "       -0.00344063, -0.00240931], shape=(1024,), dtype=float32), array([-0.07078646,  0.01539246, -0.0152958 , ..., -0.01574817,\n",
      "        0.10984577,  0.00584589], shape=(1024,), dtype=float32), array([-0.02877134, -0.00534067, -0.01736639, ..., -0.02964875,\n",
      "        0.06475408, -0.02385511], shape=(1024,), dtype=float32), array([-0.01747803, -0.01543893,  0.00348983, ..., -0.03447313,\n",
      "        0.01791835,  0.00483568], shape=(1024,), dtype=float32), array([-0.03794152, -0.028993  , -0.00103482, ...,  0.01601014,\n",
      "        0.03552765, -0.00928941], shape=(1024,), dtype=float32)], 'sparse': <Compressed Sparse Row sparse array of dtype 'float64'\n",
      "\twith 1177 stored elements and shape (12, 250002)>}\n",
      "Dense document dim: 1024 (1024,)\n",
      "Sparse document dim: 250002 (250002,)\n"
     ]
    }
   ],
   "source": [
    "from pymilvus.model.hybrid import BGEM3EmbeddingFunction\n",
    "\n",
    "bge_m3_ef = BGEM3EmbeddingFunction(\n",
    "    user_fp16=False, \n",
    "    device=\"cuda:2\"\n",
    "    )\n",
    "dense_dim = bge_m3_ef.dim[\"dense\"]\n",
    "\n",
    "docs_embeddings = bge_m3_ef.encode_documents(docs)\n",
    "\n",
    "# Print embeddings\n",
    "print(\"Embeddings:\", docs_embeddings)\n",
    "# Print dimension of dense embeddings\n",
    "print(\"Dense document dim:\", bge_m3_ef.dim[\"dense\"], docs_embeddings[\"dense\"][0].shape)\n",
    "# Since the sparse embeddings are in a 2D csr_array format, we convert them to a list for easier manipulation.\n",
    "print(\"Sparse document dim:\", bge_m3_ef.dim[\"sparse\"], list(docs_embeddings[\"sparse\"])[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"id\" : i, \"dense\" : docs_embeddings[\"dense\"][i].tolist(), \"subject\" : \"NLP\", \"text\" : docs[i]}\n",
    "    for i in range(len(docs))\n",
    "]\n",
    "\n",
    "res = client.insert(collection_name=\"demo_collection\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: {'dense': [array([-0.03501119, -0.02539117, -0.00141004, ..., -0.03410389,\n",
      "        0.0315879 ,  0.02493187], shape=(1024,), dtype=float32), array([-0.07105995, -0.00034722,  0.01634757, ..., -0.03044891,\n",
      "        0.03097286, -0.01159351], shape=(1024,), dtype=float32)], 'sparse': <Compressed Sparse Row sparse array of dtype 'float64'\n",
      "\twith 12 stored elements and shape (2, 250002)>}\n",
      "Dense query dim: 1024 (1024,)\n",
      "Sparse query dim: 250002 (250002,)\n"
     ]
    }
   ],
   "source": [
    "queries = [\"표현학습이란 무엇인가요?\", \n",
    "           \"RNN의 문제는 무엇인가요?\"]\n",
    "\n",
    "query_embeddings = bge_m3_ef.encode_queries(queries)\n",
    "\n",
    "# Print embeddings\n",
    "print(\"Embeddings:\", query_embeddings)\n",
    "# Print dimension of dense embeddings\n",
    "print(\"Dense query dim:\", bge_m3_ef.dim[\"dense\"], query_embeddings[\"dense\"][0].shape)\n",
    "# Since the sparse embeddings are in a 2D csr_array format, we convert them to a list for easier manipulation.\n",
    "print(\"Sparse query dim:\", bge_m3_ef.dim[\"sparse\"], list(query_embeddings[\"sparse\"])[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'insert_count': 12, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### S2S DEEP LEARNING APPROACH\n",
      "- 여러 번(독립적으로) Prediction(Sequence 길이만큼) + Convolution Idea\n",
      "- **Sequence를 모델링**(**순서**를 넣어보자!)\n",
      "- 입력 순서를 달아 준다. **기억과 연관**된다.(네트워크가 기억력을 가짐)\n",
      "기억 : 과거의 어떤 것이 현재에 영향을 미치는 것.\n",
      "- **Neural Network + Memory = Recurrent Neural Network**\n",
      "- 결과물이 다음 번의 입력으로 다시 들어옴.  \n",
      "- RNN의 문제(한계)\n",
      "- **Vanishing Gradient**\n",
      "- (backpropagation시)term이 길어질 때 Gradient가 소실된다.\n",
      "- 구조적으로 바로 전의 것에 영향을 받게 되어 있기 때문에 Long-Dependency가 있는 입력 혹은 결과가 잘 반영되지 않는다.\n",
      "- **Exploding Gradient**\n",
      "- 더 중요한 정보, 덜 중요한 정보를 구분하지 못하고 Gradient가 폭발할 수 있다.\n",
      "- Short-Dependency가 더 중요함에도, 멀리 있는 gradient가 너무 높게 계산 되어 가까이 있는 gradient가 충분히 반영되지 않는 경우.  \n",
      "- 기존 RNN은 sequence가 길어지면 성능이 떨어짐.\n",
      "→ LSTM이 이걸 해결하는 방법으로 나옴.\n",
      "→ LSTM RNN으로 최초의 S2S을 개발.  \n",
      "- LSTM(개념만 알아두면 됨.)  \n",
      "| neural | memory | 의미                                                     |\n",
      "| ------ | ------ | ------------------------------------------------------ |\n",
      "| input  | Write  | 1이면 입력 x가 들어올 수 있도록 허용(open). 0이면 Block(closed)        |\n",
      "| output | Read   | 1이면 의미있는 결과물로 최종 Output(open). 0이면 해당 연산 출력 안함(closed) |\n",
      "| forget | Reset  | 1이면 바로 전 time의 memory를 유지. 0이면 reset. Keep gate        |\n",
      "- 각 Gate에 현재와 과거의 정보가 모두 반영되도록 Network 설계  \n",
      "- GRU\n",
      "- Update Gate를 두어서 현재 Time의 Hidden state를 계산할 때 update gate의 영향을 받도록 함.\n",
      "- Reset Gate를 두어서 현재 Memory를 Reset할지 안할지를 결정  \n",
      "→ attention, transformer  \n",
      "- Sequence Modeling for POS Tagging : NtoN\n",
      "- S2S example - Programming Code Generation(N21)\n"
     ]
    }
   ],
   "source": [
    "res = client.search(\n",
    "    collection_name=\"demo_collection\",\n",
    "    data=[query_embeddings[\"dense\"][1].tolist()],\n",
    "    limit=2,\n",
    "    output_fields=[\"text\", \"subject\"]\n",
    ")\n",
    "\n",
    "text = res[0][0]['entity']['text']\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eahc00/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.41it/s]\n",
      "Device set to use cuda:1\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/gemma-2-9b-it\",\n",
    "    device=\"cuda:1\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "당신은 대학생의 학습을 도와주는 학습 도우미입니다. 사용자가 물어보는 질문에 대한 답을 학생이 제공한 문서에서 찾아 제공합니다. 문서에 적힌 내용만을 답하고 없으면 문서에서 찾을 수 없는 내용이라고 대답하세요.\n",
      "문서: ### S2S DEEP LEARNING APPROACH\n",
      "- 여러 번(독립적으로) Prediction(Sequence 길이만큼) + Convolution Idea\n",
      "- **Sequence를 모델링**(**순서**를 넣어보자!)\n",
      "- 입력 순서를 달아 준다. **기억과 연관**된다.(네트워크가 기억력을 가짐)\n",
      "기억 : 과거의 어떤 것이 현재에 영향을 미치는 것.\n",
      "- **Neural Network + Memory = Recurrent Neural Network**\n",
      "- 결과물이 다음 번의 입력으로 다시 들어옴.  \n",
      "- RNN의 문제(한계)\n",
      "- **Vanishing Gradient**\n",
      "- (backpropagation시)term이 길어질 때 Gradient가 소실된다.\n",
      "- 구조적으로 바로 전의 것에 영향을 받게 되어 있기 때문에 Long-Dependency가 있는 입력 혹은 결과가 잘 반영되지 않는다.\n",
      "- **Exploding Gradient**\n",
      "- 더 중요한 정보, 덜 중요한 정보를 구분하지 못하고 Gradient가 폭발할 수 있다.\n",
      "- Short-Dependency가 더 중요함에도, 멀리 있는 gradient가 너무 높게 계산 되어 가까이 있는 gradient가 충분히 반영되지 않는 경우.  \n",
      "- 기존 RNN은 sequence가 길어지면 성능이 떨어짐.\n",
      "→ LSTM이 이걸 해결하는 방법으로 나옴.\n",
      "→ LSTM RNN으로 최초의 S2S을 개발.  \n",
      "- LSTM(개념만 알아두면 됨.)  \n",
      "| neural | memory | 의미                                                     |\n",
      "| ------ | ------ | ------------------------------------------------------ |\n",
      "| input  | Write  | 1이면 입력 x가 들어올 수 있도록 허용(open). 0이면 Block(closed)        |\n",
      "| output | Read   | 1이면 의미있는 결과물로 최종 Output(open). 0이면 해당 연산 출력 안함(closed) |\n",
      "| forget | Reset  | 1이면 바로 전 time의 memory를 유지. 0이면 reset. Keep gate        |\n",
      "- 각 Gate에 현재와 과거의 정보가 모두 반영되도록 Network 설계  \n",
      "- GRU\n",
      "- Update Gate를 두어서 현재 Time의 Hidden state를 계산할 때 update gate의 영향을 받도록 함.\n",
      "- Reset Gate를 두어서 현재 Memory를 Reset할지 안할지를 결정  \n",
      "→ attention, transformer  \n",
      "- Sequence Modeling for POS Tagging : NtoN\n",
      "- S2S example - Programming Code Generation(N21) \n",
      "질문: 기존 RNN의 문제점이 무엇인가요?\n",
      "기존 RNN의 문제점은 Vanishing Gradient와 Exploding Gradient입니다. \n",
      "\n",
      "Vanishing Gradient는 backpropagation 시 term이 길어질 때 Gradient가 소실되는 현상이고, Exploding Gradient는 더 중요한 정보, 덜 중요한 정보를 구분하지 못하고 Gradient가 폭발할 수 있는 현상입니다. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"당신은 대학생의 학습을 도와주는 학습 도우미입니다. 사용자가 물어보는 질문에 대한 답을 학생이 제공한 문서에서 찾아 제공합니다. 문서에 적힌 내용만을 답하고 없으면 문서에서 찾을 수 없는 내용이라고 대답하세요.\"\n",
    "\n",
    "def format_user_prompt(query, document):\n",
    "    return system_prompt + f\"\\n문서: {document} \\n질문: {query}\"\n",
    "\n",
    "query = \"기존 RNN의 문제점이 무엇인가요?\"\n",
    "print(format_user_prompt(query, text))\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": format_user_prompt(query, text)\n",
    "    }\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=512)\n",
    "print(output[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
