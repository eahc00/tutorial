
## Transformer 이후의 발전 과정 요약
- 2017년 Transformer
- 2018년 중반 GPT
	- decoder only
- 2018년 후반 BERT
	- encoder only
- 이후 decoder only와 encoder only 모델들이 각각 발전함.

- 시간이 지날수록 transformer의 크기가 커짐.
	- **parameter수가 커진다**는 것.


### Transformer 발달의 Mega trend
- **Domain Specialization**
	- 범용 architecture에 사용하는 데이터가 domain specialization(분야 특화)으로.
	- ChemBERTa 등
- **Multi-Modal**
	- **여러 종류의 데이터**가 한 번에 support될 수 있는 것. 
	- Vit : 이미지를 패치별로 토큰화 등
- **Super-Large Model**
	- 모델의 크기가 커진다.
	- GPT2 → GPT3의 크기 발전
- **Encoder only & Decoder only**
	- (Decoder only가 활용 범위가 넓고 훈련이 쉬워서 이게 더 많이 쓰임)
	- GPT, BERT 등
- **Support Longer Sequence**
	- transformer는 내부적으로 self-attention과 같은 architecture를 사용하기 때문에 token이 많아질수록(즉, 문장이 길어질수록) complexity도 급격하게 올라간다.
	- 그래서 적당히 긴 문장을 넘어 선 엄청나게 긴 문장은 다루기 힘들다. 그래서 **내부 구조를 바꿔서 굉장히 긴 문장을 다루도록 하는 기술**이 나오고 있다.
	- LongFormer 등
- **Optimization**
	- transformer는 메모리와 속도 면에서 고비용의 architecture이다. 
	- Less Memory : **메모리** 최적화
	- fast Attention : **속도** 최적화
	- **MoE**(**Mixture of Experts**)
		- 하나의 전문화된 모델만 있는게 아니고 여러 개의 전문화된 transformer들이 있고 이걸 내부적으로 묶어서 전체적으로 하나의 시스템처럼 작동시키게 하는 것.


## Data Size & Model Size and LLM
- Test Loss
	- 일반화 loss(generalization loss)
	- unseen data에 대해 어떠한 손실값을 가지는지.


### Deep Learning Scaling is Predictable
- **model size가 클수록** 더 일반화 능력이 올라간다.
- data와 model size에 따라서 어느 정도 예측 가능한 trend(경향성)가 있다.

- **Compute, Dataset Size, Parameter와 test loss는 반비례**하는 경향을 파악.
- **token processed**
	- 모델이 얼마나 많은 **token에 노출**되었냐
	- 여러 번 학습했거나 많은 데이터로 학습 했을 때 높아짐
- **파라미터가 많으면** token processed가 높을 때 test loss가 더 잘 떨어지고 많이 떨어진다.

- training loss와 parameter
	- 데이터가 작을 수록 로스가 크다.
	- **모델 사이즈, 데이터 사이즈가 크면 클수록 성능이 더 올라간다.**
- **FLOPs**(계산량) - 얼마나 많은 계산이 이뤄졌나
	- 모델 사이즈가 커질수록 FLOPs도 커지게 된다.

- 최근엔 small language모델 
	- 작지만 전문화된 모델을 연구하는 분야도 생김.


## Large Language Model Introduction
### Language Model
- 우리 머릿속에 있는 언어의 모델을 computational(계산 가능한) 형태로 바꾸고자 하는 모든 연구는 Language model 연구이다. 
- 인간의 단어 나열을 어떻게 기계도 비슷하게 모델링할 수 있을 것인가를 연구하는 학문.

- **규칙 기반 모델링(Rule based modeling)**
	- A → B → C 와 같은 식으로 규칙화를 시켜놓는다.


#### (Probabilistic) Language model
- **확률 이론에 기반**한 language model(데이터 기반 **통계적 언어 모델**)
	- 일종의 단어 나열(sentence, part of sentence)의 확률이 그럴듯 한지 안한지를 잡아내는 것.
	- Likelihood of symbols sequences
	- 그럴듯한지 안한지를 확률로 산출

- **N-gram model**
	- 우리가 가진 문서에서 어떤 단어가 나오는 frequency를 재고 그 이후 **연속적으로 특징 단어가 나오는 frequency**를 잡아낸다.
	- ex )
		$$P_{train}(dream|have\space a) = \frac{n_{train}(have \space a \space dream)}{n_{train}(have\space a)}$$
	![[CNU/3-1/자연어처리/images/Pasted image 20240513233904.png|450]]
	- 과거에 많이 쓰였지만 여전히 유효함.


#### The brief history of Large Language Models
- 1966 : ELIZA, SHRDLU 규칙 기반 Language Model
- 1980 후반 ~ 1990 : Statistical Language Model(데이터 기반)
	- 2000년 대까지 probabilistic language model이 주로 사용됨.
- 2000년대 초반: **Neural Probabilistic Language Model**
	- Neural Network에 집어넣어서 **다음 나올 단어의 확률을 예측**하는 방식

- 2013 : Word2Vec
	- 딥러닝을 NLP분야에 적용하는 시도가 시작됨.
	- 어떤 단어를 계산 가능한 숫자 덩어리로 Converting하는 과정을 예전에는 각 단어마다의 layer를 각자 학습하는 방식으로 수행
	- 우리가 구할 수 있는 대부분의 단어에 대해 많은 계산을 통해 믿을만한 dense한 벡터를 만들어 줌.
	- Context, word, document 전체를 받아서 예측하는 방식

	→ attention & transformer (2017)


#### Types of Language Model
![[CNU/3-1/자연어처리/images/Pasted image 20240513235245.png|450]]

- **Masked LM**
	- 아무 단어를 masking하고 예측하는 방식.
	- 확률 모델링
-  **Auto-regressive LM**
	- 앞에 history(앞 단어들)를 기반하여 다음 것을 예측
	- 내가 방금 예측한 단어들을 다시 이용하여 다음 단어를 예측한다.
	- 생성한 걸 바탕으로 그 다음 것을 계속 생성.


##### Auto regressive
![[CNU/3-1/자연어처리/images/Pasted image 20240513235422.png|450]]

- autoregressive
	- 이전에 예측한 걸 다시 이용해서 \[eos\](end of sentence)가 나올 때까지 예측(좌)
- **non-autoregressive**
	- 입력을 넣어서 입력 하나로 **한 번에 생성**
- 성능은 auto regressive가 더 좋음.


#### Types of Language Model
- BERT
	- transformer
	- Encoder only
	- **masking을 사용**하여 언어 모델링
- GPT
	- Decoder only
	- **auto regressive를 사용**하여 언어 모델링
- BART
	- encoder + decoder
	- masking + auto regressive 등 여러 가지 방법으로 학습


### Large Language Model
- 얼마나 커야 Large Language Model인가?
	- 교수님 피셜.
	- number of parameteres
	- GPT-3 수준부터가
		- 개인 컴퓨터에서 돌리기 어렵다.
	- GPT-3 token의 수
		- 데이터를 공개하지 않음.
		- 개인이 똑같은 데이터로 훈련시킬 수 없다.
	- 모델이 어느 정도 **critical한 size**를 넘어가면 모델이 **추론 능력**을 가지게 되는 게 아닌가라는 얘기도 나옴.(turing machine급)
		- 70B, 30B 내외

- GPT-3 논문
	- LLM is 'few-shot' learner
	- **zero-shot**
	- **one-shot**
	- **few-shot**

	![[CNU/3-1/자연어처리/images/Pasted image 20240514000828.png|400]]
- 13B는 few-shot으로 성능이 무조건 올라간다.
- model size가 커지면 커질수록 성능이 올라간다.
- model size가 큰 모델에서, 한 개 이상의 prompt를 넣어주면 성능이 많이 향상된다. 
- 어느 정도 size에서 **critical point가 존재**하고 그 point를 넘어가면 **reasoning area(추론 능력을 가짐)** 가 된다.


#### 어떻게 하면 LLM 모델이 추론 능력을 가지게 훈련 시킬 수 있을까?
- 두 가지의 학습 방법이 있다.
	![[CNU/3-1/자연어처리/images/Pasted image 20240514001515.png|400]]
- parameter 수(model size)가 커질수록 training loss가 떨어진다.
- 데이터가 많을수록 training loss가 떨어진다.
	→ **모델 키우고 데이터 많이 모으면 된다.**(OpenAI, Google)
- but 비용이 많이 든다.

- **parameter수를 고정**한다고 하면?
	- **데이터 노출(데이터 양과 훈련 에폭)** 을 늘리면 loss가 떨어지지만 loss입장에서 별 차이가 없다는 것이다.
	- 모델 사이즈를 작게 유지하더라도 그 작은 사이즈를 미리 **학습을 엄청 많이** 해놓으면 되는 거 아닌가
	- 엄청 많이 학습을 하겠다. → 모델 사이즈 작기 때문에 개인도 훈련을 시킬 수 있다.(Meta)

> [!result]
> Train Longer → Loss decrese


### Large Language Model Based AI
- LLM 이전의 AI
	- Domain Problem → formatted input →  AI Model(classification, regression, generation) → formatted output → result
- LLM 이후의 AI
	- Domain Problem → text input →  LLM(generation) → text output
	- 사람이 이해하는 input, 사람이 이해하는 output이 튀어 나온다.

![[CNU/3-1/자연어처리/images/Pasted image 20240514002720.png|400]]

- 그냥 문장의 반을 자르고 뒷 부분을 생성하도록 한다.
- **Likelihood가 높은 문장을 생성**하도록 학습