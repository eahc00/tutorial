

## \[Introduction\]Parameter Efficient Fine-Tuning(PEFT)
- LLM 모델의 신경망, 파라미터의 개수가 점점 더 커지는 경향(초거대 언어모델)
- 우리의 GPU로는 이 모델의 훈련 자체(fine-tuning)가 불가능한 경우가 많다.
- 이때 우리의 작은 GPU에도 집어넣어서, 즉 **parameter efficient하게 만들어서 fine-tuning**하는 기술이 개발 되었다. 

### Pipeline
![[CNU/3-1/자연어처리/images/Pasted image 20240521210746.png|500]]

- Super Large Data → LLM architecture 
	- 이게 **Pretraining** 과정이다.
	- random한 파라미터부터 학습.
- 이 훈련된 결과물을 활용하는 파이프라인 두 가지
	- 이 **언어 모델 자체를 Inference server 형태**로 사용.
		- ex) chatGPT, Gemini
		- Prompt → answer
	- 언어 모델을 **내부적으로 Fine-tuning**하는 이슈가 있을 수도 있다.
		- 이 거대 모델을 로컬에 있는 환경에 넣은 다음에 내 관심 도메인의 데이터를 부어서 fine-tuning하겠다는 needs가 있다.
		- → 새로운 언어 모델을 만들어서 serving 하고 싶다.
		- 도메인 특화 서비스

> [!problem]
> - LLM의 모델 크기가 너무 커서 일반 연산 장치에서는 구동이 어렵다.
> 	- ex) GPT3-1750억 개의 파라미터
> - 어떻게 초 거대 언어모델을 내 로컬 컴퓨팅 인프라에 가져올 것이냐?

> [!definition]
> - **PEFT**(**Parameter-Efficient Fine-Tuning**)
> 	- 사전 학습된 언어 모델을 개선하여 특정 작업을 더 잘 처리할 수 있도록 하는 NLP분야의 최신 접근 방식
> 	- 아주 커다란 모델을 내가 파라미터 효율적으로 훈련할 수 있는 모든 테크닉을 PEFT 테크닉이라고 부를 수 있다.
> 	- 비교적 최근 언어모델들이 거대화 되면서 반사적으로 나타난 기술
> 	- 모델의 모든 파라미터를 업데이트하는 기존의 fine tuning 방식과 달리 PEFT는 **모델 파라미터의 일부만 선택적으로 조정**한다.
> 		- 이러한 선택적 업데이트는 **계산 리소스를 절약**하고 모델이 이전에 학습한 정보를 잃어버리는 **catastrophic forgetting(재앙적 망각)을 방지**한다.
> 	- 모델의 일부만을 건드리거나 모델의 사이즈를 줄이는 등이 있다.
> 		- 어댑터 모듈 추가, 프롬프트 튜닝, sparse updating 방식 등이 있다.
> 	- 이러한 기법은 사전 학습된 모델을 새로운 작업에 보다 효율적으로 적용하여 제한된 계산 능력으로도 미세 조정을 가능하게 한다.


### 전체 PEFT를 둘러싸고 있는 몇 가지 흐름
![[CNU/3-1/자연어처리/images/Pasted image 20240521211352.png|400]]

- **Additive** 
	- 모델을 다 건드릴 수는 없으니 처음에 가져온 모델은 그대로 놔두고 내가 control 할 수 있는(즉, backpropagation의 대상이 되는, 훈련을 할 수 있는) **layer를 추가**하는 approach
- **Adapter**
	- 위에서 설명한 layer를 **어느 위치에 추가**할 건지 → 중간중간 끼워넣는 아이디어
- **soft prompts**
	- 언어 모델이 입력을 받는 텍스트가 실제로는 임베딩 된다. 
	- **임베딩** : hard symbol → dense vector(이걸 결국 soft prompt라고 할 수 있다.)
	- soft prompt를 어떻게 잘 가공해서 언어 모델의 Power를 끌어 올릴 수 있을까
- **selective**
	- 레이어를 적당히 **잘라내서 쓰겠다**는 테크닉
- **reparameterization-based**
	- 거대 언어모델의 **파라미터 자체를 비슷하게 따라할 수 있는** 또 다른 레이어나 파라미터를 고안해서 처리하는 테크닉
	- ex) LoRA


### Ideas

#### Prompt Approach
- **"Steering Handler"** 만 바꿔보자
	- 가정 : LLM은 다양한 도메인에서 응답할 수 있는 기능을 보유하고 있지만 아직 **적절한 쿼리 방법**을 숙지하지 못했다.
	- 굉장히 큰 언어모델이 자동차라고 생각했을 때, 기존에 쓰던 wheel(운전대)을 가공해서 다른 wheel로 갈아끼우게 되면 차가 움직이는 패턴이 좀 바뀌지 않을까?
	- 이런 식으로 내가 원하는 바람직한 답변을 얻어낼 수 있지 않을까

- 구현체 (2021~2022)
	- P-tuning
	- Prefix-Tuning
	- Prompt-Tunint


#### Additive Approach
- Additional Layer
	- 가정 : 새 도메인에서 작동하는 데 도움이 되는 **추가 레이어를 삽입**하고 **이 레이어만 학습**한다.
	- 거대 언어 모델의 모든 것을 건드리지 않고 대신 내가 건드릴 수 있는 특수한 layer를 끼워넣고 이것들만 훈련하겠다.
	- **레이어와 레이어 사이에 내가 건드릴 수 있는 layer들을 추가**해서 훈련데이터로 훈련해보겠다.
- 구현체
	- Adapters


#### Reparameterization
- **Fine-Tuning용 parameter**를 작게 관리
	- 가정 : 미세조정에 직접적인 영향을 미치는 parameter를 작게 유지한다.
	- 커다란 언어모델을 모두 쓰지 않고 거대 모델을 카피를 하든 선형대수의 테크닉을 이용하여 정보량을 갖고 있는 작은 모델을 유지하든 해서 다른 것들로 reparameterization, 즉 **가중치를 흉내낼 수 있는 또 다른 가중치를 만들어내서 처리**한다.
- 구현체
	- LoRA


### Prompt Approach
- **Hard-Prompts**
	- 일반적으로 **텍스트**이다.
	- 사용자가 언어모델을 특정 방향으로 guide하거나 특정 작업을 수행하기 위해 수동으로 만든 사전 정의된 문자열 또는 문장.
		- ex) 질문-응답 모델에서 하드 프롬프트는 "프랑스의 수도는 무엇인가?"와 같은 질문이 될 수 있다.
	- symbol
		- ex) 고양이에 대한 정보 → 0, 1로 표현(있거나 없거나)
- **Soft-Prompt**
	- **vector**
	- 고정된 텍스트 입력이 아니라 **학습 가능한 매개변수 또는 임베딩의 집합**이다.
	- 소프트 프롬프트를 사용하는 모델에서 이러한 임베딩은 특정 작업에 대한 모델의 성능을 향상 시키기 위해 학습 과정에서 최적화된다.
	- 이러한 임베딩은 입력의 한 형태로 작동하지만 내부적으로 모델이 조정하고 학습할 수 있는 벡터로 표현된다. 
	- ex) 고양이가 있을 확률 → 0~1사이의 값으로 표현. 0~1사이의 상태를 가지는 것. 대체로 확률 분포에 해당.

- 따라서 **하드 프롬프트**는 **사용자가 읽고 쓸 수 있는 실제 텍스트**인 반면, **소프트 프롬프트**는 **모델이 내부적으로 응답을 guide하는 데 사용되는 숫자 벡터**이다.
- 텍스트 기반의 프롬프트는 대체로 Hard-Prompts라고 하는데, 이걸 어떤 식으로든지 벡터화 시켜서 이 벡터를 컨트롤하는 것 자체가 transformer에 들어가는 것이다.
- Hard에 해당하는 text의 정보량을 들고 있지만 미묘하게 다른 **soft prompt를 이용해서 컨트롤** 해보겠다는 게 기본 아이디어.


#### P-Tuning(2021)
![[CNU/3-1/자연어처리/images/Pasted image 20240521212834.png|600]]

- 원래는 프롬프트가 입력으로 들어가면 dense vector로 바뀌고 모델에 들어오고 다시 돌아와 전체적으로 backpropagation이 일어난다.
- P-tuning에서는 **프롬프트 자체를 Encoding**하는 게(Prompt Encoder) 하나 더 들어있어서 **프롬프트 자체(input 자체를)를 컨트롤**해서 거기서부터 dense vector를 만들어낸다.
	- 이때 backpropagation 되는 대상이 LLM모델이 아니고(LLM 모델은 parameter frozen) **Prompt Encoder**가 된다.
	- 데이터가 그때그때 필요한 요소로 바뀌게하여 성능을 올린다.
- P-tuning의 성능은 BERT와 GPT에서 3~4점 정도의 이득을 볼 수 있었다.

> [!note]
> - P-tuning은 훈련 가능한 연속적인 프롬프트 임베딩을 이산적 프롬프트와 연결하여 사용하는 새로운 방법을 제안한다.
> - 프롬프트 인코더는 주어진 프롬프트의 임베딩만을 학습하여 task를 보다 정확하게 반영하고 다른 모든 요소는 동일하게 유지한다.
> - P-tuning은 **프롬프트를 최적화하는 방법**으로, 더 큰 모델에 주어진 프롬프트를 개선하는 데 중점을 둔다. 
> 	- 이는 이 기법이 여전히 대규모 모델의 기능에 크게 의존하고 있음을 의미한다.


#### Prefix-Tuning
![[CNU/3-1/자연어처리/images/Pasted image 20240521220659.png|400]]

- 일반적으로 과거에는 어떤 inference serving을 한다고 하면 모델이 서비스별로 여러 개 존재한다.
	- 원래는 모든 모델이 훈련의 대상이 됐다. train, inference 모두에서 전부가 필요한 셈.
- Prefix-tuning에서는 **모든 모델을 하나로 통일** → **Single model**
	- 앞쪽의 **prefix를 상황에 따라서 tuning하는 기법**을 사용하므로써 prefix만 건드려도 전체 성능이 모델을 전부 다 사용하는 것보다 괜찮거나 더 좋을 수 있다.

![[CNU/3-1/자연어처리/images/Pasted image 20240521220931.png|400]]
- FT : fine tuning
- PT : prefix tuning
- full fine tuning 한 것보다 성능이 더 잘 나오는 것을 확인할 수 있다.

> [!note]
> - P-tuning에서는 학습 가능한 파라미터가 입력 임베딩에만 통합되는 반면, prefix-tuning에서는 이러한 파라미터가 **네트워크의 모든 계층에 걸쳐 추가**된다
> - 이러한 접근 방식은 모델 자체가 fine-tuning 중인 작업에 대해 더 깊이 이해할 수 있도록 보장한다.
> - prefix tuning의 성공의 핵심 요소는 입력 시퀀스를 넘어 모델의 각 계층에 학습 가능한 파라미터를 도입하여 유연성을 크게 향상시키는 확장성에 있다.


#### Prompt Tuning(2021)
![[CNU/3-1/자연어처리/images/Pasted image 20240521221108.png|450]]

- 일반적으로 하는 finetuning은 BERT와 비슷하다.
	- input text를 넣고 feed forward해서 loss 계산 하고 모두가 backpropagation 된다.
- Prompt Design
	- 아예 훈련하지 않고 Prompt Engineering을 통해서 성능을 향상시키는 것이다.
- Prompt Tuning
	- 모델은 아예 건드리지 않지만 **Input Text를 받을 때 앞쪽에 내가 교정할 수 있는, 일종의 학습 가능한 형태로 soft prompt를 만들어 집어 넣겠다**는 것이다.
	- Prefix tuning + P-tuning이 섞인 아이디어
	- 여러 가지 모델을 다 사용하지 않고 하나로 합친 상태에서 각각을 적절히 잘 튜닝하면 성능이 잘 나온다는 것.

![[CNU/3-1/자연어처리/images/Pasted image 20240521221608.png|350]]

- 프롬프트 튜닝이 전체를 모두 튜닝한 것과 결국은 유사해진다.
- 모델 파라미터가 커질수록 프롬프트 튜닝이 더 좋다.

> [!note]
> - tuning 가능한 soft prompt를 사용하여 frozen model을 conditioning하는 효율적이고 효과적인 방법을 제안한다.
> - soft prompt의 **"token"은 학습 가능한 벡터**이다. 즉, 학습 데이터셋에 대해 soft prompt를 end-to-end(한 번에)로 최적화할 수 있다.


- 위에서 core model은 frozen된 채로 새로운 학습 가능한 벡터에 이러한 gradient update를 적용한다.

![[CNU/3-1/자연어처리/images/Pasted image 20240523205631.png|400]]

- 왼쪽 : model tuning을 사용하면, 입력 데이터가 task-specific model로 라우팅 된다.
- 오른쪽 : prompt tuning을 사용하면, 여러 task의 example 및 prompt가 하나의 frozen된 모델을 통해 **일괄 처리** 되어 서비스 리소스를 더 잘 활용할 수 있다.


### Types of PEFT
![[CNU/3-1/자연어처리/images/Pasted image 20240521221727.png|550]]


## PEFT : Additive approach
- 원래의 파라미터는 그대로 두고 **small trainable module을 집어 넣겠다**는 것이다.

> [!QnA]
> - 원래 모델이 있고 additive 모델을 하나 더 추가한다고 하면 원래 모델 + additive 모델의 파라미터가 필요하니까 여전히 내 GPU에 못 넣는 거 아닌가요?
> - 원래 모델 파라미터가 많더라도 훈련 과정에 들어가면 gradient를 계산하고 저장하기 위해서 모델의 파라미터 수가 더 커진다. 
> 	- 따라서 원래 파라미터 + 더 많은 메모리가 요구됨.
> - 그래서 원래 파라미터는 그냥 내 GPU에 넣고 Additive모델에 대해서만 gradient 계산 수를 추가하기 때문에 훨씬 작아진다.


### Adapters
> [!info]
> - **Adapter는 사전 학습된 모델의 레이어 사이에 삽입되는 학습 가능한 작은 모듈**이다.
> - Fine-tuning 중에는 이러한 Adapter의 파라미터만 학습되고 원래 모델 파라미터는 frozen된 상태로 유지된다.


![[CNU/3-1/자연어처리/images/Pasted image 20240521222156.png|500]]

- Multi-Head Attention과 Add & Norm 사이에 보통 Adapter를 끼워넣는다.
	- Adapter도 design의 대상으로 여러 형태로 만들 수 있다.
	- **Adapter만 훈련(backpropagation)의 대상**이 된다.

![[CNU/3-1/자연어처리/images/Pasted image 20240521222438.png|400]]

- 파라미터 수가 커지면 커질수록 Adapter 수는 일부만 대상으로 학습하기 때문에 고르게 학습이 된다.
- ex) Adapter를 쓰지 않았을 때는 계산 과정이 한 방향으로 흘러서 다른 도메인, 새로운 데이터에서 성능이 잘 안나온다면 Adapter가 방향을 틀어주고 계산 되기 때문에 다른 결의 정보가 나올 수 있게 된다.


## \[PEFT\]Low-Rank Adaptation(LoRA)

### Reparameterization
- Idea : Parameter를 작게 관리
	- 가정 : 직접적으로 fine-tuning에 영향을 주는 파라미터들을 작게 관리한다.
	- 커다란 언어모델을 일종의 **흉내 내거나** 혹은 언어모델과 **같이 연산될 수 있는** 조그마한 관리되는(backpropagation되는) 모델을 따로 관리한다.
	- 이 조그만 모델을 선형대수의 테크닉을 활용하여 A, B 벡터(matrix)로 decomposion 되어 굉장히 작게 유지될 수 있게 한다.
		![[CNU/3-1/자연어처리/images/Pasted image 20240521232618.png|300]]
	- ex)LoRA


### LoRA
- 두 가지 아이디어를 섞어 놓음
	- Idea 1 : fine-tuning process의 재해석
	- Idea 2 : (재해석한 바탕 위에서) Parameter를 작게 관리


#### Idea 1. Fine tuning의 재해석 New form of Fine-tuning
- Parameter Update with Gradient Descent
	- 파라미터 업데이트 과정을 remind 해보면, 
	![[CNU/3-1/자연어처리/images/Pasted image 20240521232803.png|500]]
	- 손실함수(Objective function)의 저점을 찾아가는 과정
	- 가장 일반적으로 GD(경사하강법)을 이용. 
		- 미분을 하여 방향(파라미터를 어디로 이동시켜야 하는지)을 알아내어 이동한다.
		- GD는 변화량을 정하는 역할도 한다.
		- 변화량에 control factor가 들어가는데 이게 learning rate

- 위 파라미터 업데이트 과정을 $W' = W + \nabla W$로 둔다.
	- $W$ : 가중치 행렬
	- $\nabla W = -\alpha \frac{\partial L(W)}{\partial W}$

![[CNU/3-1/자연어처리/images/Pasted image 20240521233844.png|400]]

![[CNU/3-1/자연어처리/images/Pasted image 20240521234018.png|300]]

- 수식적으로 3번 과정이 동일하다.
	- 입력 x를 양쪽에 다 넣어준다.
	- pretrained weights는 그대로 놓고 $\nabla W$만 업데이트하면 fine tuning이 된다고 생각할 수 있다.


> [!question]
> - 어쨌든 LoRA에서 추가되는 레이어는 랜덤에서 시작하는 거 아닌가? 어떻게 W의 변화량 값을 가지도록 학습할 수 있나? 그냥 이론적으로 그렇게 학습 된다는 거?
> - LoRA에서 Wieght Update되는 레이어는 그럼 fine tuning하려는 task에 적합한, 기존 가중치에 더해지는 gradient 값을 weight로 가지게 되는지?
> - 근데 weight 값은 사람이 정해주는 게 아닌데 그냥 결과적으로 이렇게 될 것이다 하고 예측하는 거?
> 	- $\nabla W$를 가지게 된다는 결과


#### Idea2. Parameter를 작게 관리 Low-Rank Decomposition
> [!question]
> - original parameter weight W, 변화량 parameter weight 두 개의 파라미터가 존재하는데, 그럼 파라미터가 \*2 가 되는 거 아닌가요?
> 

$$\nabla W = A * B = W_A *  W_B$$
- original parameter weight는 학습이 굉장히 안정적으로 되어 있다(고른 파라미터).
- 고른 파라미터를 가지고 와서 domain에 맞게 fine tuning을 하는데 사실 이 변화량 W는 그렇게 변화가 많지 않을 수도 있다는 것. 
	- 왜냐면 domian 데이터가 pretrain 데이터보다 적기 때문에 **변화량 W는 생각보다 작을 수 있고 그렇기 때문에 변화량 W를 축소 시킬 수 있을 거**다.
- 어떻게 축소?
	- **A, B를 $W_A$, $W_B$로 나눠서 접근**하겠다는 것.
		![[CNU/3-1/자연어처리/images/Pasted image 20240521235430.png|400]]
	- r setting 값에 따라서 A, B 메모리의 양이 굉장히 작아질 수 있다는 것.
	- r이 중간에서 interface 역할을 해줄 수 있다.
		![[CNU/3-1/자연어처리/images/Pasted image 20240521235543.png|300]]


##### Choosing the Rank-r : 메모리와 성능 사이의 trade-off가 있다.
- r가 높아질수록 정보량이 많아지므로 성능이 올라갈 수 있지만 **메모리를 많이 할당**해야 한다.
- 반대로 r이 낮아지면 메모리는 낮아지겠지만 **성능은 어느 정도 손해**를 보게 된다. 
- 이 중간에 적절한 값을 찾는 게 우리의 engineering 포인트가 된다.

> [!info]
> - rank r이 낮을수록 덜 복잡한 low-rank 행렬이 생성되어 Adaptation phase에서 학습해야 하는 파라미터 수가 줄어든다.
> 	- 이렇게하면 학습 프로세스를 가속화하고 요구되는 계산량을 줄일 수 있다.
> 	- 하지만 r이 감소하면 task-specific한 부분을 캡슐화하는 행렬의 능력도 감소하여 adaptation의 효율성이 저하될 수 있다.
> 	- 결과적으로 이 모델은 더 큰 r을 사용할 때보다는 새로운 작업에서의 성능이 저하될 수 있다.
> - 요약하자면, LoRA에서 낮은 r을 선택하려면 모델의 단순성(simplicity), 적응성(adaptability), underfitting 또는 overfitting의 위험 사이에서 균형을 맞춰야 한다.
> - 따라서 다양한 r 값으로 실험하여 의도한 작업 성능을 달성하기 위한 최적의 균형을 맞추는 것이 중요하다.


##### Initialization of $W_A$ and $W_B$
- $W_A$ Initialization
	- **random Initialization** : 일반적인 파라미터 세팅할 때 쓰는 평균 0, 적절한 분산값(편차)의 normal distribution에서 뽑은 값으로 초기화한다.
	- 이러한 randomness는 **대칭(symmetry)을 깨는 데 도움**이 되며 모델이 학습 초기 단계에서 다양한 특징을 학습할 수 있도록 한다.
	- $W_A \sim N(0, \sigma^2)$

- $W_B$ Initiallizaton
	- 보통 **Transfer leaning, slow start**에서 출발하는 개념으로 Initializatoin
	- 잘 학습되어 있는 X를 가져와서 X'가 되길 원하는데 데이터의 결이 많이 다르다면 weight 값이 fully finetuning 되기 때문에 좋은 가중치 정보를 잃어버리게 된다.(재앙적 망각, Catastrophic forgetting)
	- 이를 방지하기 위해 **slow start를 도입.**
		- 처음에는 과거의 가중치를 강하게 보고 나중에는 전체 가중치를 강하게 보는 구조
		- 바로 모든 걸 parameter update하지 않고 (frozen) head만을 backpropagation 대상으로 두고 학습을 한다. → 거칠었던 head가 좀 잔잔해진다.
		- 이후 이전 레이어까지 또 fine tuning시킨다. → 이런 식으로 **한 레이어씩 모든 레이어를 학습**시킨다.
		- → 원래 내가 받았던 pretrain model의 처리 능력도 그대로 유지하면서 내 새로운 데이터에서까지의 성능도 보장 받을 수 있다.
	- 이러한 측면에서 $W_B$를 **Zero Initialization**
	- $W_B = 0$
	 $$\nabla W = W_A * W_B$$
	- 처음에 위 식에서 $W_B = 0$이면 처음에는 $\nabla W$가 없는 것처럼 작동을 한다.
	- 그러다가 여러 step을 돌다보면 $W_B$도 배우게 되므로 slow start와 비슷하게 동작할 수 있다. → **안정적으로 fine tuning이 가능**해진다.


![[CNU/3-1/자연어처리/images/Pasted image 20240522005844.png|200]]


#### LoRA @ inference
- LoRA를 가지고 Inference하는 것을 생각해보자.
- inference를 하게되면 overhead가 있는 것처럼 보인다. 
	- pretrained weight와 LoRA 두 개로 입력이 들어가므로 두 개의 pipeline이 있기 때문에.

- 학습을 다하고 inference를 하게되면 **weight를 합치면 된다**. → $W_{merged}$
	![[CNU/3-1/자연어처리/images/Pasted image 20240522010114.png|150]]
	![[CNU/3-1/자연어처리/images/Pasted image 20240522010142.png|150]]
- huggingface에 `merge_and_unload()` 함수를 이용하여 이 과정을 자동으로 수행해준다.
	- 위 함수를 사용하여 adapter 가중치를 base model과 병합하면 새로 병합된 모델을 독립형 모델로 효과적으로 사용할 수 있다.

> [!info]
> - LoRA는 Adapter 가중치를 base model과 병합할 수 있기 때문에 inference 대기 시간이 늘어나지 않는다.


#### How LoRA is Applied
- **Identify Target Layer**
	- 전체 파라미터가 아닌 **어느 레이어에 LoRA를 적용**할 건지를 정할 수 있다. → Target Layer
- Insertion of Low-Rank Matrices
	- LoRA에 대한 파라미터(위에서 $\nabla W$)를 집어 넣어야 한다.
	- LoRA는 레이어의 모든 가중치를 fine-tuning하는 대신 **선택한 레이어의 각 가중치 행렬 W에 대해 두 개의 low rank 행렬 A와 B를 도입**한다.
	- 기존 가중치 행렬 W는 훈련 중에 직접 수정되지 않는다.
- Low-Rank Update
	- original operation $Wx$ → $(W +AB)x$ 
	- 기존 W가 아닌 **LoRA의 $W_A$, $W_B$만이 LoRA의 rank update 대상**이 된다.
	- 어떤 r을 쓸 건지가 중요. → **Memory & Performance trade-off**
	- Backpropagation동안, A와 B만이 update된다.
- Training/Fine-Tuning


#### Memory Savings
- 얼마나 메모리가 세이브 되는가?
> [!example]
> ![[CNU/3-1/자연어처리/images/Pasted image 20240522010719.png|350]]

- Transformer에서 더 큰 D가 된다면 엄청나게 Ratio가 엄청 떨어진다. (압도적인 사이즈의 메모리)


#### Advantage
- transformer 기반의 엄청난 파라미터 수를 가지고 있는 모델에 굉장히 적용이 잘된다. 특히 **Attention block** 같은 데에 적용이 잘 되어 성능이 잘 된다.
- r 값도 다양하게 설정이 될 수 있는데, 만약 r을 8로 설정한다면 LLaMa 7B모델이 23GB → 8MB만 쓰면 되게 된다.
- 성능 또한 괜찮다.
	![[CNU/3-1/자연어처리/images/Pasted image 20240522011137.png|400]]
- GPT-3에서 175B → 37.7M가 된다.
- 전체를 가만히 놔두고 조그만 변화량만을 가지고 학습 시키기 때문에 작은 데이터에 맞게끔 민감하게 학습되어 성능이 더 잘 나올 수 있다.


#### 참고 : intrinsic dimensionality
- $W_A = [D, r]$로 정의 되는데 r(rank)을 선형대수 관점에서 말해보면
	- 선형대수에서 어떤 데이터를 포함하고 있는 커다란 matrix(정보량)이 있다고 하면, 정보량 자체를 표현하거나 핵심을 뽑아낼 때 모든 차원이 필요하지 않다.
	- 내부적으로 **필수적인 intrinsic한, 내부적인 deimension이 존재** 한다는 것이다. 

> [!example]
> - 그래서 원래는 data가 1,000차원이라고 하면 1,000차원짜리 data가 10,000개가 있다면 10,000개의 데이터를 관통하는 핵심 정보는 10차원만 있어도 충분하다는 것이다.
> - 이 10차원이라는 게 LoRA에서 rank가 되는거고 이게 intrinsic dimensionality이라는 것.
> - 이런 테크닉의 일종으로 PCA가 있다.


##### PCA and intrinsic dimensionality
- 많은 수의 고차원 데이터를 뿌려봤을 때 이 모든 데이터를 관통하는 차원 수들 중에 주 차원수, **Pricipal한 주성분에 해당하는 component**가 있다는 것.
	
![[CNU/3-1/자연어처리/images/Pasted image 20240522012145.png|450]]

- 2~3개의 정보(차원)를 가지고도 전체 데이터의 핵심 feature를 설명할 수 있다는 것.

> [!note]
> - LoRA에서 학습을 시킬 때 전체 모델을 표현하는 차원이 필요가 없다는 것이다. 
> - 이 작은 수의 데이터에 맞게 파라미터가 업데이트 돼야하는 intrinsic한 차원수가 있을 거고 그게 아주 작을 거라는 것.


#### LoRA 개선 테크닉
![[CNU/3-1/자연어처리/images/Pasted image 20240522012716.png|450]]

- Original LoRA
	$$h' = h + \alpha(LoRA\space output)$$
	- 이때 여기서 LoRA를 통해 계산한 값에 곱해지는 $\alpha$값이 LoRA alpha다.
	- 실제로는 lora_alpha에서 r값을 나눈 값이 들어간다.
- **Rank-Stabilized LoRA**(rsLoRA)
	- `lora_alpha / math.sqrt(r)`
	- `use_rslora` 
	- $\sqrt{r}$이 성능이 더 좋다고도 한다.

![[CNU/3-1/자연어처리/images/Pasted image 20240522013032.png|400]]

- rsLoRA가 조금 더 부드럽게 학습됨을 보인다.


## \[PEFT\]Automatic Mixed Precision and Quantization
### Mixed Precision Training (NVIDIA)
- BLOOM-176B을 inference하기 위해서만 PEFT 테크닉을 쓰지 않는다면 8장의 80GB A100 GPU가 필요하다.
- 훈련을 한다면 약 72장의 GPU가 필요하다(9배의 메모리가 필요).
- 훈련을 감안하면 기존의 날것의 언어모델이나 대규모 파라미터를 그대로 쓰는 것은 불가능하다.


#### Float Memory Storing
- 메모리 구조
	![[CNU/3-1/자연어처리/images/Pasted image 20240522121855.png|400]]

- 32bit, 64bit 공간을 어떻게든 숫자를 표현하기 위해 나눠야 하는데 현재 표준은 IEEE-754 표준을 사용한다.
	- sign bit가 한 비트 사용된다.
	- **Exponent(지수)** : 8비트로 표현할 수 있는 정수가 10의 지수로 올라가서 곱해진다. → **Range(범위)** 결정
	- **Singificand** : 앞에 계수 부분이 나머지에 들어간다. → **Precision(정밀도)** 결정
- 엄청나게 큰 숫자를 표기하려면 당연하게도 Exponent의 비트수가 많으면 많을수록 표현할 수 있는 수가 커진다.
- 수의 정밀도를 높이고 싶으면 Significand의 영역의 비트수가 많아져야 한다.
- 따라서 **Exponent(큰 수, 작은 수)** 와 **Significand(정밀하게, 수의 해상도)** 가 **Trade-off의 관계**를 가진다.
	- 정해진 비트수를 나눠 써야 하기 때문에


#### Less Memory
- PEFT : 어떻게든지 메모리를 줄이면 되는 것이다.
- 32bit로 한 숫자를 표현하던 거를 만약 16bit, 8bit로 숫자 하나를 표현하면 어떨까
- 단순하게 숫자를 표현하는 메모리 투자 방법을 아주 간소화시켜서 **같은 수를 표현하더라도 적은 메모리를 사용**하게 된다면 어찌됐든 Parameter Memory Efficient한 것이니까 이렇게 해보겠다.


##### Less Memory in ML
- 원래의 방법 : FP32
- 이걸 반절로 줄인 것 : FP16
	- **Exponent 5 bit + Significand 10 bit**
	- 최대가 $10^5$이 된다. **10,000 ~ 100,000정도 밖에 표현**이 안되는 것. range가 줄어든다.
	- 내가 담고 싶은 숫자(parameter) 값이 100,000을 넘는 값이 있다면 이거를 FP16으로 표현하려고 하면 표현이 안된다는 단점이 있다.
	- 이러한 문제를 해결하기 위해 나온 방법이 두 가지가 있다.
		- BF16, TF32
- **BF16**
	- Exponent 8bit + Significand 7bit
	- Exponent는 FP32와 같다. **큰 수, 작은 수는 똑같이 표현**하고 **정밀도를 좀 떨어뜨려서** 너무 상세하게 보지는 않겠다는 게 BF16.

![[CNU/3-1/자연어처리/images/Pasted image 20240522133414.png|400]]

- 파라미터 update 과정을 생각해보면 작은 변화량 * learning rate(대체로 1e-3에서 더 작아질 수도 있다.)
	- 따라서 **작은 값을 표현**해야 되고 **동시에 작은 값의 해상도도 중요**한 게 **parameter update** 과정.
	- 그래서 **Optimizer 과정** 중에는 **FP32를 사용**한다.(안정적 + 위험하지 않다)
- Feed Forward 과정에서는 생각보다 큰 수, 작은 수가 중요하지 않고 세밀한 해상도가 필요 없다는 것이다. 중요한 건 Backpropagation 과정에서 중요하다는 것.
	- 따라서 **feed forward 과정, 미분 계산하는 과정까지는 FP16**을 쓰고 **파라미터 update하는 back propagation 과정에서는 FP32를 사용**한다. 
- 이 과정을 자동으로 change 해주고 싶다는 게 **Automatic Mixed Precision(AMP)** 라고 한다.
- 즉, 큰 메모리를 작은 메모리로 넣어서 학습해주고 싶고 그러기 위한 옵션 중에 하나가 AMP다.


###### How to use AMP
- Pytorch + Huggingface
![[CNU/3-1/자연어처리/images/Pasted image 20240522133557.png|400]]


### Quantization 양자화
- 초거대 언어모델을 학습시키기 위한 핵심 기술.
- **숫자의 format을 바꿔보겠다**는 idea.
	- **실수를 정수로, 정수도 8bit 정수나 4bit 정수로** 바꿔보겠다는 것.

- Quantization의 Two Types 
	![[CNU/3-1/자연어처리/images/Pasted image 20240522134049.png|350]]


#### Zero-Point Quantization 
![[CNU/3-1/자연어처리/images/Pasted image 20240522134144.png|400]]

- 0점을 중심으로 우리가 표현하는 숫자가 -1.0에서 1.0 값이고 이걸 정수로 표현할 때 정수의 존재 위치가 -127에서 127 사이라고 하면 -1에서 1사이 값을 양자화 시킬 때 최댓값을 곱해준다.
	- 위 사진에서 0.3 * 127
- 이걸 소수 부분을 잘라서 integer로 바꿔준다.
	- 38
- 복구하는 과정(dequantization 과정)에서 127로 나눠주는데 0.2992가 나온다.(원래는 0.3)
	- 그렇다면 0.0008값은 날라간 것. 그렇다면 Optimization 과정에서 굉장히 작은 값들이 미묘하게 적용될 수 있기 때문에 문제가 생길 수 있다.
	- 이 때 이렇게 날아가는 값(0.0008)을 **degradation**이라고 한다. 손실이 일어난 것.


#### absmax quantization
- **absmax quantization**이라는 게 나왔다. 
	![[CNU/3-1/자연어처리/images/Pasted image 20240522141105.png|400]]
	
	- **절댓값**을 가지고 한다 : content base
	- 우리가 quantization해주고 싶은 FP16 vector가 있다면 이 벡터를 scanning하여 절댓값이 가장 큰 값을 찾는다. 
	- **절댓값이 가장 큰 걸 이용해서 전체를 quantization** 시킨다.
		- ex) 최댓값이 5.4라면 이걸 이용해서 전체 벡터를 quantization 시키는데, 5.4를 정수 최댓값인 127로 만들어준다. 이때 5.4 → 127로 만들기 위해 **곱해지는 값을 quantization factor**라고 한다.(127 / 5.4 = 23.5를 곱하면 된다.)
	- quantization factor를 모든 벡터에 곱해서 정수화한다.
	- 이후 dequantization 진행. (정수화한 값에서 quantiztion factor를 나눠줌.)
	- 이것도 degradation이 발생하지만 content를 보면서 하는 거기 때문에 zero-quantization보다 훨씬 안정적인 값이 나온다.


##### Paper from Hugging face
- [논문](https://arxiv.org/pdf/2208.07339)
- 초거대 언어모델도 **int8로 Quantization**할 수 있는 기술 개발 + 거의 zero degradation이다.  
- scanning하는 과정에서 문제가 생길 거 같은 수를 outlier로 정의하고 **outlier를 제외하고 absmax quantization**하는 방법.

![[CNU/3-1/자연어처리/images/Pasted image 20240522141606.png|300]]

- 작은 모델부터 큰 모델까지 FP16으로 학습을 하다가 어느 순간 2.7B ~ 6.7B 사이에 있는 모델은 8bit로 줄인 순간 **성능이 훅 떨어지더라.**
- 왜? quantization 입장에서 **문제가 생긴 값들이 존재**할 것인데, 그걸 outlier라고 잡고 풀어보겠다.
- **matrix를 여러 개로 쪼갰다가 붙이는 과정**으로 풀어낸다.

![[CNU/3-1/자연어처리/images/Pasted image 20240522142359.png|400]]

- Matmul에서 **outlier 값을 노란색 줄로 표현**한다.
- outlier에 해당하는 값을 따로 **분리해서 빼낸다.** (outlier의 column과 곱해지는 row를 같이 분리해서 빼낸다.)

![[CNU/3-1/자연어처리/images/Pasted image 20240522142528.png|400]]

- 이런 식으로 분리하고 분리한 과정을 기억하여 다시 복원도 되도록 한다.
- ex) 위 사진에서 **노란색에서는 수가 크고 굉장히 큰 absmax**를 가진다. → degradation가 날 수 있고, 일단 수가 큰 노란색 값들을 outlier라고 치고 배제를 시켜놓는다.
- (outlier가 빠진) 남은 값들을 가지고 어떻게 quantization을 수행할 것인가?
	![[CNU/3-1/자연어처리/images/Pasted image 20240522142818.png|500]]


- **Quantization Part**
	- matrix $X$에서 column-wise하게 absmax를 찾는다. → 이 벡터를 $C_X$
	- W에서는 row-wise하게 absmax를 구해 $C_W$를 구하고 이걸 이용하여 Quantization을 진행
		$$X_{F16} * (\frac{127}{C_X}) = X_{I8}$$
	$$W_{F16} * (\frac{127}{C_W}) = W_{I8}$$
	- 위에서 $(\frac{127}{C_X})$, $(\frac{127}{C_W})$가 quanitzation factor.
	- 이렇게 나온 값은 **안정적**일 것.
		$$X_{I8}W_{I8} = Out_{I8}$$
	- $Out_{I8}$을 복구(복호화)시킨다.(de-quantization)
		$$\frac{Out_{I8} * (C_X \otimes C_W)}{127 * 127} = Out_{F16}$$

- **Outlier Part**
	- 그냥 메모리를 줄이지 않고 **그대로 이용**한다. ($Out_{F16}$) 
	- reduce degradation

- 결론적으로 이 두 값을 기억하고 있던 방식으로 다시 조합하여 최종 output을 산출.

 - Performance
	 - OPT-175B으로 돌렸을 때
		 ![[CNU/3-1/자연어처리/images/Pasted image 20240522144225.png|400]]
	 - BLOOM-176
		 ![[CNU/3-1/자연어처리/images/Pasted image 20240522150040.png|400]]
	 - 그냥 FP16과 int8로 했을 때와 성능이 별 차이가 나지 않는다.
	

###### threshold
1. input hidden state에서 outlier를 뽑아내야 한다. 이 **outlier는 특정 threshold보다 높은 값을 가지는 것**이고 이걸 마킹해둔다.
2. **outlier는 FP16**으로, **outlier가 아닌 것들은 Int8로 matrix multiplication**한다.
3. 이후 복호화하여 FP16으로 다시 병합한다.

![[CNU/3-1/자연어처리/images/Pasted image 20240522150400.png|450]]
- llm_int8_threshold를 **6.0**을 기준으로 설정해놓았다.
- hidden state를 가지고 특정 threshold 위에 있는 걸 다 조사해 봤을 때, 여기서 보통 대부분의 값은 normal distribution을 따르는데 대부분 -3.5와 3.5 사이에 존재한다.
- 그런데 예외적으로 outlier가 존재하고 대체로 -60에서 6, 6에서 60의 값들이 문제가 되더라. 그래서 threshold를 6으로 설정.
- 더 낮은 threshold를 적용해볼 수 있지만 그러면 불안정한 모델이 될 수도 있다.


###### How to use LLM.int8() @ Pytorch + Huggingface
- quantizaton이라는 것은 메모리는 유지가 되는데 메모리를 쓰는 방식에 있어서 실수를 정수로, 정수도 8bit, 4bit짜리로 줄여나가는 것. 
	- 이러한 관점에서 최근 기술은 손실 없이 quantization을 잘 해보자하는 테크닉도 나와있다.

![[CNU/3-1/자연어처리/images/Pasted image 20240522153011.png|400]]

- hugging face에서 모델을 load할 때 `load_in_8bit`옵션을 켜주면 quantization을 적용하여 불러오기 때문에 메모리를 차지하는 게 굉장히 작다.