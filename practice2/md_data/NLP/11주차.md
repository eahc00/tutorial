## Big Neural Network Training Tricks
### Gradient Clipping

![[CNU/3-1/자연어처리/images/Pasted image 20240610183604.png|600]]
 
- gradient update를 위해 아래와 같은 수식이 나온다. $$\theta_{new} = \theta_{old} + \alpha \nabla$$
	- $\alpha$ : control factor(0~1)
		- gradient가 얼마나 적용되길 원하는지

- 이 때 만약 **gradient가 너무 크면** 어떻게 할 건지?
	![[CNU/3-1/자연어처리/images/Pasted image 20240610183843.png|400]]
	- loss가 smooth하게 내려가는 게 아니라 진동하여 튀는 현상이 생긴다.
	- 이걸 만회하기 위해 **gradient clipping** 사용.
		- 너무 큰 값을 허용하지 않고 자른다.
	- optimizers 선언 시 `clip_value`인자를 설정하는 식으로

> [!example]
> ![[CNU/3-1/자연어처리/images/Pasted image 20240610184220.png|400]]


### Learning Rate Change
- 파라미터는 내 모델의 능력을 결정 짓는다.
- 바깥에서 결정(내가 정해주는) 파라미터가 Hyper param.
	- 바깥에서 파라미터가 학습되는 결, 방향을 바꾸도록 영향을 주는 요소.
	- ex) Learning rate, Batch size, weight decay, transform(이미지) 등
- **hyper-parameter tuning**이 필요.


![[CNU/3-1/자연어처리/images/Pasted image 20240610184431.png|500]]

- learning rate가 너무 작으면 학습 속도가 느림.
- 너무 크면 loss가 떨어지지 않고 튀어버린다.
- **중간에 적정한 learning rate를 찾아줘야** 한다.


#### Step Decay
![[CNU/3-1/자연어처리/images/Pasted image 20240610184501.png|500]]
- **계단 함수**를 사용하여 learning rate를 떨어트린다.


#### warmup Learning Rate
![[CNU/3-1/자연어처리/images/Pasted image 20240610184813.png|600]]

- 내가 커다란 머신러닝 모델을 가지고 있다고 가정.
- 초기 파라미터는 랜덤 초기화이기 때문에 굉장히 거칠 것이다. 
	- 거친 상태에서 Loss를 계산하면 이때 나오는 gradient 또한 거칠 것이다.
	- 이러한 처음의 gradient를 그대로 적용하는 건 좀 무리가 있다.
	- 따라서 **0부터 learning rate를 천천히 키워가서** 우리가 원하는 setting 값에 이르기까지 천천히 올라 가는 learning rate를 쓴다.

- **Cyclic Learning Rate**
	- 똑같이 **warm up**했다가 **step decay** 아이디어를 추가해서 learning rate를 떨어트린다.
	- 떨어트렸다가 천천히 올리고 떨어트렸다가 천천히 올리고를 반복.


#### Cosine Annealing Learning Rate scheduler
> [!definition]
> - Annealing
> 	- 달궈진(높아진) 값을 완만하게 떨어뜨린다. 
> 		- 달궈진 쇠를 가만 놔둬서 식는 것과 비슷한 원리
> 	- 굉장히 활성화 된 상태에서 가만 놔둬서 평형 상태에 도달할 지점까지 가는 것

> [!reference]
> - SGDR paper

![[CNU/3-1/자연어처리/images/Pasted image 20240610185248.png]]

- 위 사진은 -1 ~ 1에서의 값을 0 ~ 1로 만드는 과정.
- 결론적으로 1부터 0까지 완만하게 떨어트린다.


#### GPT3의 learning rate scheduling
![[CNU/3-1/자연어처리/images/Pasted image 20240610185343.png|600]]

- 초기에는 0부터 천천히 learning rate를 올려서 **warmup하는 방식** 사용
	- warmup_token(375M token)에 도달할 때까지
	- 모델이 너무 크기 때문에 epoch단위로 도입할 수 없다. token 기준으로. 
- 올라간 learning rate를 **cosine decay로 낮추는데**, 올라갔던 learning rate의 **10% 값을 가질 때**까지 떨구고 이후에는 같은 learning-rate를 유지한다.

> [!additional explanation]
> - 구현 Pseudo code
> 	![[CNU/3-1/자연어처리/images/Pasted image 20240610185524.png|500]]


### L2 Regularization and Weight Decay
- 머신러닝의 idea와 지금의 LLM을 학습 시킬 때의 idea가 어떻게 연결되는지.

- idea : We need small models!
	- 같은 현상을 모델링하는 거라면(같은 task를 하는 모델이라면) **가능한 작은 것이 좋다.**

> [!reference]
> - occams's razor
> 	- 가장 쉽고 가장 단순하고 가장 군더더기 없는 것을 설명


#### Object function
- 우리는 훈련 과정 중에 내 모델이 얼마나 적절한지를 알아내야 한다.
- 결론적으로 어떤 loss(Object function)를 내야하는가
 $$L = TASK + complexity$$
	- TASK 수행 평가 loss(기존 loss)
		- 주어진 task를 얼마나 잘 수행하고 있냐?
		- ex) classification, regression 등등
	- **complexity** : 모델의 복잡도에 대한 loss를 준다. 이게 규제항
		- 이 모델이 얼마나 simple하냐
		- 이걸 regularization이라고 한다.
	- 이 두 가지 다 줄어들어야 하기 때문에 task 오류도 줄어들고 complexity도 줄어드는 방향으로 학습되도록 된다.

> [!info]
> - L1 norm : manhattan distance $||X|| : |x| + |y|$
> - L2 norm : euclidian distance $||X||_2 = \sqrt{x^2 + y^2}$

- idea : **복잡함을 잡고 싶다**. → **L2 norm**을 사용하여.
	- 모든 파라미터 제곱하고 더하면 이 안에 얼마나 많은 파라미터가 있는지, 얼마나 큰 값을 가지는지를 잘 측정할 수 있다.
	- 또한 같은 파라미터 수를 가진 모델이 있다고 했을 때, 하나의 파라미터만 튀는 모델보다 모든 값들이 다 **균일한 기여도**를 가질 수 있다.
		-  하나의 값만 튀지 않고 모든 파라미터가 균일한 값을 가지도록.(**모든 파라미터가 가능한 작은 값**을 가지게 되므로.)

- Regularized Loss funtion
	$$L_{reg}(W) = L(W) + \frac{\lambda}{2}\Sigma_i w_i^2$$
	- 미분을 하기 위해 규제항에 $\frac{1}{2}$를 곱해준다.
	- $\lambda$ : regularization의 control factor

> [!result]
> - regularization의 목적은 **모델을 가능한 심플하게 유지**하면서 **전체 데이터를 설명할 수 있게끔 강제**해 주는 것.


### Weight Decay
$$\theta_{new} = (1-\beta)\theta_{old} + \alpha\frac{\partial L(\theta)}{\partial\theta}$$
- $\beta$를 사용하여 control하겠다는 것. 
	- $\beta$ 값이 0이면 그대로 적용. $\beta$ 값이 커지면 $\theta_{old}$가 작아지도록 한다.

- $L_{reg}(\theta)$를 미분하고 gradient에 대입
	- 같은 수식이 나온다.
	![[CNU/3-1/자연어처리/images/Pasted image 20240610191532.png|400]]

- **weight decay의 역할**과 (수학적으로) 정확히 **regularization의 역할이 같다.**
	$$\theta_{new} = (1-\alpha \lambda)\theta_{old} - \alpha \nabla_\theta L(\theta_{old})$$
	- $\lambda = \frac{\beta}{\lambda}$


> [!example]
> ![[CNU/3-1/자연어처리/images/Pasted image 20240610191632.png|400]]
> - weight decay를 조절하는 parameter가 존재. == 이게 결국 regularization

> [!warning]
> - regularization을 그대로 사용하게 되면 파라미터 수가 늘어나는 만큼 제곱값이 더해져야 한다.
> 	- 트랜스포머, LLM 등 큰 모델에서는 합이 몇 조 개가 될 수 있다는 것.
> 	- 그럼 이 값이 우리 컴퓨터(ex -64bit 컴퓨터)로는 표현이 안된다.
> - 그래서 이런 똑같은 효과를 weight decay를 사용해서 얻을 수 있다.(간접적으로)
> - 근데 이건 SGD, Adam 등과 같은 1차 미분 식에서만 성립. 하지만 다른 optimizer를 쓰더라도 결은 비슷하다.
> - 또한, bias는 주로 weight_decay에서 제외한다.

> [!example]
> ![[CNU/3-1/자연어처리/images/Pasted image 20240610192405.png|500]]
> - 직접 decaying하고 싶으면 위 코드를 참고 : parameter group을 넣어준다.


## ChatGPT Training Method

> [!warning]
> - 이건 추정이고 바뀔 수도 있는 내용.
> - 계속 발전되어 가고 있다.

> [!problem]
> - chatGPT 모델의 크기는 어마어마하게 큼. 이걸 돌릴 수 있는 인프라가 없기 때문에 뒤에 있는 클라우드를 이용하여 서버에 데이터(요청)를 보내서 응답을 받는 방식을 사용 : 이걸 API방식, 클라우드 방식이라고 함.
> - cloud API로 chatGPT를 이용할 때, 내 데이터나 정보를 노출하게 되는 보안 문제가 발생한다.
> - 어떻게 나도 chatGPT같은 언어모델을 바닥부터 학습 시킬 수 있을까?
> - GPT는 open되어 있는 기술이 아니다.
> 	- GPT 3.5 이후 OpenAI는 이에 관련한 논문을 내지 않음.
> 	- 마지막으로 나온 InstructGPT라는 논문이 GPT 기술의 근간이 아닐까? → 추정

> [!info]
> - action ⇄ 보상 or 체벌
> - 강화학습 (Reinforcement Learning)
> 	- 보상(reward) 기반. 
> 	- human feedback
> - 교사 학습(Supervised Learning)
> 	- 체벌(penalty) 기반 : Loss
> 	- 학습 또한 Loss를 최소화 하는 방향으로 학습한다.


- 강화 학습에는 reward model이라는 게 있고, 강화학습은 **보상 체계가 명확할 수록 성능이 좋다.**
	- 게임에서 강화학습이 되게 잘된다. (**닫힌 환경**에서 : reward 통제 가능)
	- ex) 알파고 등 
- reward model이 없는 모델이 있다. 보상 체계가 명확하지 않은 **열린 환경**이다.
	- 명확한 (잘함 vs. 못함)을 일관되게 평가하는 teacher가 없다. 
	- 특히 자연어처리, 언어 쪽에서 그렇다.

- 따라서 언어 쪽에 일관된 보상을 주는 **teacher(또 다른 신경망)를 먼저 만들고 이걸 이용**하겠다는 것.

- ChatGPT goals
	1. 말이 되게 말할 수 있도록 만든다. → Language Model → Supervised Learning
	2. 물어보는 말에 잘 대답할 수 있게 만든다. → Reward Model → Reinforcement Learning


#### Make machine fluent in language : Language Model
- **문서를 완성하는 형식**(문서 중심)
	- Documentwise 하게 학습
	- 반 잘라서 뒤쪽을 만들도록 학습. 그럴듯한 문서를 완성시키도록.
	- base(Backbone) 언어 모델을 만든다.(그럴 듯한 말을 잘함)
- 질문-응답은 잘 못한다.

> [!problem]
> - 언어 모델의 환각 증상

- **Question and Answer** : 추가 학습
	- 질문-응답할 수 있도록 한 번 더 학습
	- Q-A, instuction dataset을 모아서 부어준다.
	- 이때 Decoder는 **확률적으로 답변을 결정**하기 때문에 질문을 던질 때마다 다른 답을 줌
	- 나온 답들을 가지고 human eval 진행 → 더 좋은 답변을 정한다.(ranking, 순서를 매김)
	- **ranking된 데이터를 가지고 Reward Model을 학습**한다.
		- ranking을 잘 맞추면 penalty가 없다.
		- ranking을 잘 못 맞추고 역전시키면 역전된 만큼 penalty를 준다.
		- 이 Reward Model이 사람들이 쌓아둔 human feedback data에 대해 어떤게 좋다 나쁘다를 판별할 수 있을 정도의 성능을 가지게 되면 여기까지가 supervised learning으로 학습한 것이다.
			- 이 모델이 일관된, 고정된(더 이상 학습하지 않음) teacher가 되는 것.

- 답을 평가하는 teacher가 있으므로 GPT가 답을 내면 **teacher가 평가(점수를 준다)** 하여 **Reinforcement learning(보상으로)** 진행.
	- **PPO(Proximal Policy Optimization)** 방식으로 강화학습한다.


![[CNU/3-1/자연어처리/images/Pasted image 20240610194034.png|600]]


#### ChatGPT를 학습하는데 필요한 Datasets
- Pre-training Datasets
- LLM Fine-Tuning Datasets : Q-A dataset(instruction datasets) 
- Human Feedback(ranked datasets)
	- 가장 비싼 데이터
	- Open 되어 있는 데이터가 없다.
	- Open reward 모델이 있다. 이걸 사용하면 됨.

> [!info]
> - [Google Flan](https://github.com/google-research/FLAN)
> 	- 기존의 NLP task들(ex : GLUE, superGLUE)을 문서화(template) 해주는 project

> [!etc]
> - [DPO](https://huggingface.co/docs/trl/main/en/dpo_trainer)
> - [ORPO](https://huggingface.co/docs/trl/main/en/orpo_trainer)





## Term Proj
- 주제 없다.
- 구현 테크닉도 상관 없다.