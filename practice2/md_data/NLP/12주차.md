## \[Introduction\]T5 Model
### Introduction
- LLM : 요약, QA, 번역
- Llama 같은 거는 훈련을 시킬 수 없다.
- T5
	![[CNU/3-1/자연어처리/images/Pasted image 20240613222052.png|500]]
	- google에서 발표
	- [논문](https://arxiv.org/abs/1910.10683)
	- transformer의 Transfer Learning의 끝을 탐색해보겠다는 idea
		- 하나의 모델로 다 해보겠다.(번역, 요약, 분류, 분석 등...)


### Prefix Input formatting
- **"\[Task-specific prefix\]" : "\[Input text\]" → "\[output text\]"**
- 이러한 입력 formatting은 모든 자연어 처리 작업을 텍스트 생성 문제로 변환하는 데 사용되는 방법이다.
- 각 작업에 특정 접두사가 할당되어 입력 데이터를 모델이 기대하는 형식으로 변환하는 데 사용된다.
	- 이 접두사는 모델이 수행해야 하는 작업의 유형을 알려준다.

> [!my_question]
> - GPT 같은 거는 생성 task인데.. 이거는 데이터에 있는 document에서 QA 방식을 이용하는 건가?
> 	- 11주차 GPT 학습 방법 참고


### Pretraining
> [!review]
> - BERT(encoder only) : token 분류
> - GPT(decoder only) : autoregressive 방식을 이용하여 pretrain
> - BART(encoder - decoder 방식) : T5와 비슷한 방식으로 학습

- **Denoising masked token**
	- 문장의 일부를 X, Y 등으로 마스킹 시킨 뒤 모델에게 X, Y에 들어갈 말을 예측하게 한다.
	![[CNU/3-1/자연어처리/images/Pasted image 20240613222530.png|500]]

- Pretrain
	- BERT 정도 size의 **transformer(encdoer, decoder)를 사용**한다.
	- **C4 데이터셋** 사용
		- C4 문서에서 <\M>으로 표시되는 masked token을 채우는 방법을 학습한다 : Denosing masked token

### finetune
![[CNU/3-1/자연어처리/images/Pasted image 20240613222556.png|500]]

- 추가 정보나 문맥을 입력하지 않고도 질문에 답할 수 있도록 미세 조정했다.
	- T5가 사전 학습 중에 내재화한 '지식'을 기반으로 질문에 답한다.
- 여러 task로 훈련시킨다.
- 하나의 모델로도 **prefix를 사용하여 전체를 커버하는 모델**을 만들 수 있다.




## N2M 자연어 문제 모델링 및 분석
### N2M Problem
- N개의 input으로부터 M개의 output을 구하는 문제
### Task Example
- Doc이 입력으로 들어간다 할 때
	- Translation
	- Summarization
	- Question-Answer

> [!etc]
> - 요즘에는 특히 다 이걸 사용한다.

> [!review]
> - decoder only
> 	- 문서의 앞부분이 들어올 때
> 	- 가장 그럴 듯한 문서의 뒷부분을 완성하는 것.
> - 약간 애매하지만 이것 또한 N2M이라고 할 수 있다.

### Text Summarization
#### Extractive Summarization
- **추출 요약**
- 문서에서 가장 중요한 것을 추출해내어 요약 수행.

> [!my_question]
> - Binary Classification 이용?

#### Abstractive Summarization
- **추상(생성) 요약**
- 모델이 document를 이용하여 **생성**을 하는 것.

##### train
-  fine-Tuned Model
- 원문과 사람이 요약한 **gold summary를 가지고 fine tuning**시킨다.
	- 원문이 Encoder
	- 요약문이 Decoder
	- 요약은 여러 개일 수 있다.

- **Instruction-Tuned Model : Decoder only** (최근의 방식)
	- 원문 + 요약문이 하나의 **single document**로 바꾸는 것.
	- 이 두 개를 하나로 넣어주고 decoder only 모델에게 위에서부터 쭉 읽어나가면서 그대로 summary를 생성하도록 하는 것.

> [!etc]
>  - FLAN


### Model Architecture
- Sequence-to-Sequence Model
	
	![[CNU/3-1/자연어처리/images/Pasted image 20240613222807.png|500]]


- 인코더가 디코더에 정보를 넘겨주는 데, 인코더의 맨 끝 벡터를 올려준다.
- 디코더에 RNN을 붙인다.

> [!warning]
> - 모델을 학습할 때 문장이 끝나지 않고 계속 같은 말이 반복 된다면
> - 문장이 끝나야해서 eos의 확률 분포가 더 많이 커야하는데, 그렇지 못하면 eos가 생성되지 못하고 문장이 끝나지 않고 같은 말이 반복되게 된다.

> [!my_question]
> - 이런 문제를 해결하는 방법으로는 뭐가 있는지?
> 	- 가장 대표적인 해결 방법으로는 그냥 max token 길이 지정해주는 방식이 있다.


#### Encoder-Decoder Model
- KQV → Encoder에서 온 K, V로 **cross attention** 방식을 사용하여 **blending**해서 이용
- Encoder는 한 번만 작동
- Decoder는 eos가 나올 때까지 여러번 작동한다.

> [!note]
> - K, V
> 	- time step1 : Key값은 이미 encoder에서 결정
> 	- time step2 : time step1이 한 번 더 계산
> 	- time step3 : time step 1, 2가 한 번 더 계산
> - 이미 했던 연산을 다시 하는 낭비가 발생
> - **캐싱하는 방식**을 이용
> 	- L1, L2 캐싱
> 	- cpu + memory(DDR, RAM 등)
> 	- cpu에 굉장히 빠르고 고속인 l1 memory를 둔다.
> 	- 그 다음으로 l2 memory(조금 더 느린)를 둔다.
> 	- 반복적으로 lookup하는 걸 캐시에 올려둔다.
> 	- 성능이 크게 향상
> 	- kv caching라는 키워드로 검색


#### Autoregressive vs. Non-Autoregressive
- 과거의 정보를 다 이용하여 다음 것을 예측
- autoregressive 방식을 요즘 대체로 이용.
	- 근데 **처음(bos가 들어 왔을 때) 예측을 잘못하면 다 잘못**될 수 있다.
	- **처음 예측을 잘하면 뒤 단어도 잘 예측이 될 가능성이 높다.**
	- 문장이 길어질수록(생성이 되면 될수록) 가능한 단어수가 적어진다.
	- 뒤로 가면 갈수록 **확률 분포가 편향**된다.
	- 모델 입장에서는 뒤로 가면 갈수록 고르기가 편해진다.


##### Training
- 세 가지 데이터가 사용
	- Encoder input
	- <s\> + B : Decoder input
	- B + \</s\> : Decoder output

- 이런 식으로 각 turn마다 loss가 계산되고 문장 생성 이후(eos가 나온 후) 전체 로스를 합하여 계산


#### Decoder Only Model
- input sequence → 이후 단어 예측
- 나온 단어를 붙여서 또 input sequence + 이전 나온 단어 → 이후 단어 예측
- 이런식으로 autoregressive 이용.

- Encoder의 vocab과 Decoder의 vocab이 다르게 구성된 경우가 많았는데, Decoder only를 쓴 이후 **Encoder와 Decoder가 vocab을 공유**해야함. 둘 다 공유할 수 있는 **큰 크기의 vocab이 사용**된다. → 이러한 tokenizer들이 나옴.
- Embedding Layer 또한 학습의 대상. → $W_E$의 위치와 Embedding vector의 id가 대응된다.


#### Output Postprocessing
- Convert Vocabulary ID
- Output Token Vectors $\simeq$ "Logit" → softmax를 이용하여 확률 분포로. → 가장 큰 값을 합쳐서 Convert Vocabulary ID로 다시 복원 → Output Text로 바꿔준다.


##### Search Strategy
- **Deterministic Search**
	- 특징 : 재현성, 예측 가능성(규칙 기반)
	- **Greedy Search**
		- 매 turn마다 가장 좋은 걸 선택(Greedy).
		- 문제 : 문장이 길면 길수록 앞 선택이 아쉬울 수 있다.
			- 확률을 생각하지 않고 가장 좋은 것만 선택. **나머지는 다 기각** 시키므로
	- **Beam Search**(폭이 있다는 것. 넓게 봄.)
		- Greedy에서 폭을 넓힌다.
		- 선택한 것에서 **더 여러 개를 보면서 간다**.(여러 개를 동시에 들고 간다.)
		- 이렇게 eos를 가면 여러 개의 선택지가 나온다. → 1-best, 2-best, 3-best ... n-best가 나온다. 여러 개의 output을 낸다.
	- 큰 틀에서 auto regressive
		- 주의할 것 : 이 n-best가 global n-best는 아니다.
		- 이미 기각된 게 더 좋은 path로 갈 수도 있지만 이미 기각된 구조. 

- **Sampling**
	- 특징 : 다양성, 유연성
	- **Random Sampling** : uniform 확률. 
		- 근데 확률 분포에 기반한 random sampling을 할 수도 있다. ex) gaussian sampling
	- 다음 단어가 나올 확률에 대해 이 확률 분포로 sampling을 한다.
		- ex) $P(w_a)$ = 0.6, $P(w_b)$ = 0.3, $P(w_c)$ = 0.1이라고 할 때 $w_a$가 0.6의 확률로 뽑힌다는 것.
	- **Top-K Sampling**
		- Top k개 안에서만, 이 안의 확률 분포로 Sampling을 진행한다는 것.
	- **Top-P(Nucleus) Sampling**
		- **누적 확률**
			- 앞에서의 확률을 다 더한다.
		- 누적 확률이 P미만인 것만 자르겠다.
			- ex) P = 0.9이면 상위 몇 개의 단어 확률을 더했을 때 0.9보다 작은 때까지의 상위권만 잘라내어 sampling

- 'huggingface decode generate'라는 키워드로 검색
- [참고](https://huggingface.co/blog/how-to-generate)
- [함수](https://huggingface.co/docs/transformers/v4.41.3/en/main_classes/text_generation#transformers.GenerationMixin.generate)

- **Temperature** option
	- softmax와 연관 있다.
	- sofrmax 함수의 지수를 T로 나눈다. (logit값들을 T값으로 나누고 softmax를 적용하는 것.)
		$$\frac{e^{\frac{B}{T}}}{e^{\frac{A}{T}} + e^{\frac{B}{T}} + e^{\frac{C}{T}}}$$
	- 값들의 우열은 변하지 않지만 **softmax의 y값이 줄어들게 된다.**
	- 나눠먹는 여지가 많아지기 때문에 한 값만 올라가는 게 아니고 여러 값들도 값을 올려준다.(좀 더 **평탄한 확률분포**를 만들어줌.)
		- 항상 **일관성 있는 답을 원하면 T값이 낮아야**
		- **여러 값을 다양**하게 얻고싶으면 **T값이 높아야** 한다.


### Loss Function 
#### Cross Entropy
![[CNU/3-1/자연어처리/images/Pasted image 20240613223037.png|500]]

- 단어끼리의 **확률 테이블 입장**에서 보면 된다.
	- 내가 생성한 단어의 embedding ⇄ 원래 정답(one-hot)
	- 내가 생성한 단어 분포 A, 정답 확률 분포 B 사이의 **KL divergence가 줄어드는 방향으로 학습**
		- $KL_{A→B} \neq KL_{B→A}$  
		- 이때 이 **방향성을 무시**하도록 바꿔놓은 게 **Cross Entropy**이다.
		- $i$ : 정답에 대한 인덱스
		- $p(x_i)$ : 정답 확률 값
		- $q(x_i)$ : 모델 예측 확률 값
	- 이게 한 turn씩 계산된다. → 이걸 summation하고 batch로 평균낸다.

> [!my_question]
> - 정답은 one-hot vector??
> - embedding vector로 나오는 거 아닌가?
> 	- ㄴㄴ 이거는 vocab 단어들 중에 하나로 각 turn에서 output된 거를 분류하는(선택하는) 문제로 볼 수 있어서 one-hot vector로 계산된다.



### Evaluation
- **ROUGE**(Recall-Oriented Understudy for Gisting Evaluation)
	- 요약 task에서 사용.
	- 얼마나 겹치냐
- **BLUE**(Bilingual Evaluation Understudy Score)
	- 문장이 짧으면 짧을수록 penalty
	- **n-gram precision**
	- n : 몇 개까지 겹치는 지 보겠다는 것.
