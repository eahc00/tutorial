
# Transformer based Model : BERT and GPT

## Introduction - Pretraining, Fine-tuning
- **Small-Scale Tagged Data**
- **Large-Scale Raw Data** 
	- Raw Data에 대해서 기계한테 추가적인 노력을 기울이지 않더라도 기계가 스스로 알아서 배워낼 수 있나?
	 : **EASY & FREE TASK**여야 한다. : Self-supervised learning
	→ 이걸 가지고 성능을 어느 정도 끌어올리는 모델을 만들고(**Pre-training**)
	→ 이 모델을 기반으로 Small-Scale Tagged Data를 더 사용하면 높은 성능을 얻을 수 있다.(**fine-tuning**)

![[CNU/3-1/자연어처리/images/Pasted image 20240425112830.png|400]]

## **Encoder only** - BERT
- [BERT논문](https://arxiv.org/pdf/1810.04805)
	- Encoder of Transformer
	- **Bidirectional** = Global encoding
	- Pretraining + Fine-tuning
	- State-of-the-art(at 2018)

- **Transformer의 인코더만을 사용** : 그냥 갖다쓴 건 아니고 input, output을 customized해서 사용했다.
- **Bidirectional하게 인코딩**한 것을 강조. NLU(Natural Language Understand)(생성문제가 아님)에서는 BERT가GPT보다 낫다는 게 이 논문의 결론.

- 왜 좋냐?
	- **Pretrain + Fine tuning 컨셉**이 잘 적용됐다. 먼저 많은 데이터로 선행학습을 잘 함.(CORE weights of transformer encoder를 잘 학습)
	- 그 당시 LARGE dataset으로 Self-supervised learning을 적절하게 이용했다.(Enable to use available raw data)
	- 그 뒤에 따라오는 fine-tuning task에 대해 범용적으로 잘 적용될 수 있는 모델(구조)을 제안했다.(General architecture) : TRANSFORMER(N2N, N21, Compare ...)

### Masked Language Model
 - input에 **Mask token**을 넣는다. → mask에 어떤 단어가 들어가야 하는지?(self supervised learning)
- 2개의 문장을 가져온다. 
	-  같은 document에서 나온 연속된 문장 A, B를 뽑거나 아예 다른 문장을 갖고와서 연속된 문장인지 아닌지 분류하는 문제를 낸다(맥락을 파악하도록).
	- raw data이지만 맥락을 이해해야 맞출 수 있는 문제를 만들어서 내어서 모델을 똑똑하게 만든다.



### BERT - Customization
#### Embeddings
- 하나의 input이 들어오면 Embedding이 똑같이 수행이 되는데 Segment Embedding과 Position Embedding이 추가가 된 것.
- **Segment Embedding**(BERT Embedding에서 추가된 것) : **각 문장을 구분**해주는 것 (SEP토큰까지 나눠줌) : SEP 토큰값까지 $E_A$가 더해짐. 그 뒤에 B문장에는 $E_B$가 더해진다.
- Position Embedding : 원래 Transformer에서는 Position값이 determinant하게 값이 정해진다. 이것 자체도 더해지는 $E_{position}$이 학습되도록 만든다.
→ 따라서 하나의 Input token(symbol)을 표현하는 벡터가 총 3개의 벡터(Token Embedding + Segment Embedding + Position Embedding)가 더해지는 방식으로 구성이 된다.

![[CNU/3-1/자연어처리/images/Pasted image 20240501004811.png|450]]

#### Pooler
- 원래 encoding파트에는 pooler가 없는데 pooler가 추가됨.
- BERT pooler는 **인코딩에서 문장 전체의 의도를 뽑아내도록 하는 것.**
- **\[CLS\]토큰** : 내부적으로 transformer가 attention에 기반한 것. 각각 attention한 상태에서 그 weight를 이용해서 그 전체를 하나로 blending하는 것. → **전체 문장의 요약, 추상 버전을 가지고 있다는 뜻**이다. **top layer에서의 \[CLS\] vector는 문장 전체의 의미를 담고 있는 거라고 생각할 수 있다.** 
→ 이걸 가져다가 하나의 벡터를 output할 수 있고 그 벡터는 **sequence 자체를 인코딩한 벡터**가 되기 때문에 이걸 이용하여 **downstream task(classification, regression, ...)을 진행**해볼 수 있다.

### BERT Application 활용
- BioBERT : 바이오
- ChemBERTa : 화학(입력이 화학 구조 식 : 이것도 char의 sequence로 생각할 수 있다.)
- DciBERT
→ document든, sentence든 인코딩을 해서 vector로 만들어야 하는 상황이 오면 BERT를 가져다가 쓰면 된다.

- ViT(Vision Transformer) : 비전 분야에 적용


## Decoder only - GPT
- **GPT 특성상 문장 끝에 가야 벡터가 튀어나온다**. → 앞쪽부터 방향성이 있는 것처럼 여겨진다(실제로 없지만).
	→ Bidirection이 아닌 것처럼 보인다.

- **생성에 굉장히 강하다.(Decoder 특성)**
- [GPT1 논문(2018)](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [GPT2 논문(2019)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [GPT3 논문(2020)](https://arxiv.org/pdf/2005.14165)

### Decoder - Autoregressive Generation
- 내가 처음에 모델 입력을 넣어주면 그 결과에 따라서 그 **결과가 다시 입력으로 들어가서** 그걸 기반으로 또 예측을 수행. → 내 결정을 history로 삼아서 또 결정하고 이걸 반복한다.
- 결과로 나온 것들이 돌아서 사용된다고 해서 **autoregressive**라고 한다.
- 대부분의 Decoder가 이 방식을 사용.

![[CNU/3-1/자연어처리/images/Pasted image 20240501005000.png|300]]

### GPT2 : model modification
- transformer에 대해 GPT1에서는 그대로 사용하려 함.
- GPT2는 몇가지 변경사항이 있다.

- **Layer Normalization의 위치를 변경(Pre-Layer Normalization technique)**
- Layer Normalization을 추가함.
- Initialization method를 변경함.

#### Transformer.Decoder → GPT1
- Transformer에서 Encoder를 떼고 Decoder에서 Encoder와 연관된 부분도 뗀다.
- 거의 인코더와 비슷하게 생겼다. Self-Attention에서 Masked Multi Head self-Attention으로 바뀜.

#### GPT1 → GPT2
- Layer Norm이 Feed Forward, Masked **multi self-attention전으로 내려간다** : **Pre-Layer Normalization technique.**


### How decoder works
- original transformer에서 decoder에게 \<sos\>토큰을 주어 문장 생성 시작을 알리고 \<eos\>로 생성의 끝을 알린다.
- decoder만 있는 모델을 생각하면(Decoder only의 Inference) : **처음에 start symbol이 오면 바로 디코더에서 단어 생성 → 생성한 단어까지를 넣고 output 생성 : 이걸 반복한다.**
- 결과(정답)이 start symbol로 인해 입력에서 오른쪽으로 shift돼서 간다. : **shifted right input** - 보통 이렇게 훈련, 예측을 하는 게 decoder의 훈련 방식이다.

### Prompt based classification with GPT
- 실제로 GPT에서 어떻게 문장 전체를 대표하는 벡터를 뽑아낼 수 있을까
- 문장에 **뒤쪽**에 문장을 대표하는 정보가 오게된다. 


### GPT Application model
- GPT : 생성 → Sequence를 output할 수 있다.

- chatGPT
- codeGPT
- ChemGPT


# \[overview\]Performance Metric in Machine Learning

- Machine Learning의 목적이 무엇인가?
- 내가 만든 목적을 평가하는 명확하고 정확한 기준이 필요. 어떻게 평가할 수 있을까?
- \[Measurement\]How successfully the goal is achieved?
- Application의 목적에 따라 달라진다.

### AI - Output Types
#### Categorical Information
- output = $C_k$
- Classification Metrics


#### Numerical Information
- output = number
- Regression Metrics


#### Multiple Output Items
- output = $[C_1, C_3, C_0, C_1]$
- Sequence output metrics
- Information Retrieval metrics

#### Input-Like Data
- 어떠한 input I가 들어왔을 때 결과물도 I와 비슷하게 생긴 output을 만드는 것.
- ex) 이미지 복구 등
- Restoration Metrics

#### Segmented items
- 구분 지어주는 역할을 한다.
- Clustering metrics : Unsupervised learning
- Detection metrics : Supervised learning


## Classification Metrics
- Classification의 정량적 평가


### Two Application Types
두 가지로 분류할 수 있다.

- 우리가 분류 해놓은 **결과(Class)만을 활용.(Hard Decision)**
	- Confusion Matrix
	- Accuracy
	- Precision
	- Recall
	- F1 score

- 분류할 때 어떤 **점수, 확률(Probability)** 을 가지고 이런 결정을 했는지 **추가적인 정보를 활용**하는 것.(**Soft Decision**) → 또 다른 결정을 Support 해줄 수 있는 근거로써 score가 나온다. 
	- ROC
	- AUC


#### Confusion Matrix
![[CNU/3-1/자연어처리/images/Pasted image 20240501010424.png|450]]

- **Accuracy**(정확도) = (정확한 예측 개수)/(총 예측 개수)
- Error = (부정확한 예측 개수)/(총 예측 개수)
- **Precision**(정밀도) = (실제로 TRUE인 것의 수) / (시스템이 TRUE라고 예측한 개수)
	- 시스템이 POSITIVE라고 말한 것 중 정답이 얼마나 되나?
- **Recall**(재현율) = (실제로 POSITIVE라고 말한 것의 수) / (실제로 POSITIVE인 것의 수)
	- 실제 POSITIVE 중 몇 개나 잘 잡아내는가?
	- Recall = Sensitivity = Hit Rate = Probability of Detection
- **F1 Score** : Recall과 Precision을 같이 보고 싶다. → 둘의 조합평균 이용.
		$$F_1 = \frac{2}{\frac{1}{recall} * \frac{1}{precision}} = 2 * \frac{precision * recall}{precision + recall} = \frac{tp}{tp+\frac{1}{2}(fp + fn)}$$

- **Specificity(특이도)** = (TN) / (TN + FP) → **TRUE NEGATIVE RATE**(negative를 negative라고 말할 확률)
- **Sensitivity(민감도)**(=Recall) = (TP) / (FN + TP) → **TRUE POSITIVE RATE**


- Accuracy의 문제
	- 클래스가 불균형 하다면 편향이 일어날 수 있다.
	- 만약 negative가 훨씬 많으면 모두 negative라고 하면 accuracy가 올라간다.


#### ROC and AUC
- **binary classification에 특화** 돼있는 measure
- rader에서 나옴.
- ROC curve는 **True Positive Rate(TPR)** 을 그리고 **False Positive Rate**을 다양한 threshold를 통해서 그릴 수 있는 Plotting 방법을 말하는 것.
- 그리고 이것을 다양하게 해석하면 sensitivity, recall, probability of detection 등을 알아낼 수 있다.
- **x축 : False Positive Rate(못한 것), y축 : True Positive rate(잘한 것)**
- 좋은 시스템일수록 False positive는 낮아지고 True positive는 높아진다. (BETTER 방향으로) 
	- 잘 만들어진 시스템은 PERFECT CALSSIFIER와 RANDOM CLASSIFIER의 사이에 있을 것이다.
- **이때 ROC 곡선 아래의 면적으로 성능을 측정한다. → AUC(Area Under ROC)**
- **ROC(curve plot) → AUC(score)**

![[CNU/3-1/자연어처리/images/Pasted image 20240501010228.png|400]]

#### Precision-Recall trade off
- 파란색 Decision boundary를 왼쪽, 오른쪽으로 옮긴다고 생각해보자. Decision을 내리는 선(threshold)을 좌,우로 움직이면서 시스템이 어떤 결과를 내는지를 볼 수 있는 그래프가 ROC curve
	- FP는 올라가고 FN는 떨어짐(왼쪽으로 움직였을 때).
	- FN은 떨어지고 FP는 올라감(오른쪽으로 움직였을 때).
	![[CNU/3-1/자연어처리/images/Pasted image 20240426151623.png|300]]
	- 진단이 어려운 영역에 대해 **trade-off**가 일어난다.
	- FN, FP 영역(진단 어려움)의 영역이 작을수록 이상적인 시스템
	- 완벽한 Classifier는 FP가 0인 것. TP가 가장 큰 영역(1)을 차지하는 것.
	- 일반적인 Classifier는 노란색 커브를 따른다. 
	- 어떤 시스템의 Threshold를 정할 수 있다. 의사 결정 Positive의 threshold를 확률에 기반해서 정할 수 있다. 이게 Decision Boundary이다. 

- threshold를 낮추는 경우.
	- 대부분의 경우 실제 N임에도 P라고 예측한다.
	- Recall(=True Positive Rate, = Sensitivity)는 매우 높아진다.
	- 반대로 N임에도 N이라고 말할 수 없게 된다.
	- True Negative Rate(TNR)은 떨어진다.
	- TPR, FPR이 모두 높으므로 ROC 커브의 오른쪽 위쪽 방향에 있는 것

- threshold를 높이는 경우는 반대.

#### AUC score 해석
- 0.9 이상이면 아주 좋은 것(굉장히 잘 구분).
- 0.7 미만이면 다시 고려해야 하지 않을까.
- 0.5가 Random point

