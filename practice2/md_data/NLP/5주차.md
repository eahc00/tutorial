
# Transformer
[Encoder-Decoder 참고](https://cpm0722.github.io/pytorch-implementation/transformer)

## Encoder
### Normalization
- Normalization은 **학습 속도를 향상**시킨다.

#### Layer Normalization vs. Batch Normalization
![[CNU/3-1/자연어처리/images/Pasted image 20240430135107.png|500]]
- [Normalization 참고](https://kwonkai.tistory.com/144)

- BatchNorm → **feature 단위**로 mean과 std를 계산하여 정규화를 실행
- LayerNorm → **data sample 단위**로 mean과 std를 계산하여 정규화를 실행

### Residual Connection
- CV보다 NLP에서 gradient vanishing/explosion이 쉽게 일어난다.
- **x를 한번 우회해서 넘겨주는 게** 성능이 좋다. 
	- 처음에는(학습 초기에는) weight 자체가 거칠기 때문에 잘못된 정보가 올라갈 수 있어서 우회에서 쓰다가 어느 정도 학습이 되면 그때부터 좋은 정보가 안정적으로 올라갈 수 있다. 


#### Transfer learning

> [!definition]
> transfer learning
> 
> 이미 똑똑하게 한 쪽 domain 엔진을 만들었다. 이 모델을 받아다가 다른 쪽 domain에 쓰고 싶다. 이 엔진을 가져와서 쓰고싶다. 

- 근데 다른 쪽 데이터를 가져와서 쓰다가 원래 좋았던 것까지 깨져버리고 학습된 걸 잊어버린다. (카타스트로피 러닝 Catastrophe learning, 재앙적 학습) 
- ex) CV에서 ImageNet을 이용하여 학습 시킨 모델을 transfer learning으로 사용하고 싶다. **맨 마지막 layer만을 학습 시키는 방법**을 제일 많이 사용한다. (= **Slow Start**). frozen 시킨 layer들이 기존의 기억을 잊어버리지 않게 된다.
	- 거칠지 않고 부드럽게 다른 새로운 쪽의 지식으로 넘어가도록 시키는 것.

- 이 transfer learning이 Residual Connection의 기법과 유사한 아이디어.

==> transformer는 fine tuning이 용이하다.

### Positional Embedding
- Attention은 단지 모든 information(정보)를 blend한다.
	- positional information을 잃을 수 있지만, NLP에서 단어들의 position 정보는 꽤 중요하다.
	- 상위 레이어에 positional information을 전달하기 위해서, residual connection은 필요하다.
- 우리는 Sequence를 다룬다.
- attention으로 blending되기 때문에 순서가 상관이 없어진다. 
	- 좋긴한데 순서를 어떻게 넣어줘야?
	- **Global Attention**을 가진다. 뒤에 있든 앞에 있든 다 Attention으로 봐주는 것.

##### How to make positional information?
- **position에 해당되는 고유한 벡터**를 만들자. (id처럼) 
	=> positional encoding method
	- Attention에서는 cos, sin함수를 사용.
- unique position embedding 값이 domain별로 같게 나온다. 따라서 이 방법은 이제 쓰이지 않고 다른 방법들이 많이 나왔다.
	ex)  BERT와 같은 건 domain 별로 embedding 값을 배움,

==> input embedding + positional embedding(elementwise하게)

> [!summary]
> positional embedding matrix가 없다면 트랜스포머는 각 단어의 relative position(상대적 위치)를 알 방법이 없다. 입력 문장을 무작위로 섞는 것과 같다. 따라서 positional embedding을 사용하면 모델이 입력 문장의 실제 sequential ordering을 학습할 수 있다.


참고 
1. token length 문제
2. quadratic($n^2$)한 operation이 필요하다. 이 cost를 감당할 수 없다. -> 최적화 : architecture와 parameter를 유지한 상태에서 어떻게 cost를 낮출 것인가?



## Decoder
##### What will be the query, key and value for the decoder?

![[CNU/3-1/자연어처리/images/Pasted image 20240430213503.png|100]]
- 처음은 Attention은 밑에서 올라오는 정보(Q, K, V)가 복제 or projection 돼서 만들어지는 것.
- 아래쪽 Multi-Head Attention의 Q, K, V는 모두 같다.(x) → **self attention**
- query는 자기 자신에게 오지만 k, v는 인코더(밖)(Encoder output)에서 온다. -> **cross attention**


### Masked Attention-Head Attention
##### training step
- **Teacher forcing**
	- **디코더에 실제 정답 시퀀스(Ground truth)를 입력으로** 준다.
- 디코더에 입력을 넣어줄 때는 강제로 앞에 start symbol(\<sos\>)을 넣어주고, label을 넣어줄 때 끝에 end symbol(\<end\>)을 넣어줌. 
- 그럼 순서가 하나씩 밀린다. →  **shifted right output**

##### inference step
- 디코딩 시작 때 start symbol(\<sos>)을 넣어준다. -> 디코더가 생성.
- **디코더에서 생성된 정보**를 다음 입력 때의 추가로 같이 넣어준다.
- end symbol(\<eos>) 때까지

#### There is two types of masking

##### Pad Mask
- 우리는 학습을 시킬 때 batch wise하게 수행한다.
- 한 **batch 내에서 문장 길이**가 다를 수 있다. → padding symbol사용
- 이때 **padding된 토큰에 대한 정보**를 줘야한다. 그래서 추가로 주는게 pad mask.

##### Look-Ahead masking
- 훈련 과정에서 start symbol을 넣어서 단어가 생성이 돼야한다.
- 과거 정보는 self-attention으로 걸려야하지만 미래 과정은 attention에 걸리면 안된다.
	→  **과거 정보는 봐도 되지만 미래 정보는 보면 안됨. 이걸 해주는 게 look-ahead masking.**
- 역삼각형 모양의 Masking 사용.
![[CNU/3-1/자연어처리/images/Pasted image 20240501000901.png|200]]

> [!note]
> - 인코더에서는 Padding Masking이 사용된다.
> - 디코더에서는 Look-Ahead masking과 Padding masking이 모두 사용됨.


##### Self-Attention with Look-Ahead Mask

- **Self Attention** : 입력된 sequence 전체를 대상으로 삼아서 self query로 만들어냄
- **Masked Self Attention** : 나 기준으로 앞쪽의 정보만 사용하여 attention. (생성된 것 까지만 본다)

- 일반적인 상황
	- Queries * Keys = Score
- 여기서 upper triangular 부분에 $-\infty$를 넣는다. 
- 왜? attention score는 softmax output이기 때문에 0으로 수렴한다.
![[CNU/3-1/자연어처리/images/Pasted image 20240501001031.png|400]]


- 시점을 중심으로 뒤 쪽을 masking 시키는 건 같기 때문에 pad나 look-ahead나 둘 다 그냥 -inf로 바꿔서 softmax를 하면 된다.


## Transformer를 NLP에 어떻게 적용할 수 있나?
#### 전통적인 자연어처리 vs. 최근의 자연어 처리
- 전통적인 자연어 처리에서는 내가 다룰 수 있는 구조로 바꿔서 문제를 푸는 게 일반적인 패턴.

![[CNU/3-1/자연어처리/images/Pasted image 20240501153121.png|400]]

- 언어학
	- 자연어 → 사람이 이해하고 있는 format으로 바꾸자. (품사, 형태소, 의존구문분석 등)
- 응용
	- NER

![[CNU/3-1/자연어처리/images/Pasted image 20240501153333.png|400]]

- N21 Problem
	- 감성분석, 자연어이해, 주제분류
- N2N Problem
	- 자연어이해, 형태소분석, 개체명분석
- N2Path Problem
	- 의존구문분석, 주어복원
- N2M Problem
	- 번역, 대화


## Pretraining + Finetuning
- Large-Scale(RAW) Data --Easy Task--> Pretraining Model -> Model -> Finetunig Model + Small-Scale Tagged Data -> Final Model
- 이렇게 하면 성능도, 속도도 훨씬 개선된다.

표준화
1. pretrain된 걸 배포 -> 표준화 (녹음 다시 듣기;;)
2. 누군가 TF로 모델 배포 -> 내가 torch를 쓰고싶으면 conversion을 해야.
	이걸 해주는 대신 나한테만 모델을 배포해라. 이게 허깅페이스.

