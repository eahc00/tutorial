  
Q1. cls token이 어떻게 쓰이는지
Q2. transformer
- self attention?
Q3. cross attention
## NLP 101 - Overview -
  
## Pytorch Lightning Hello World
- `torch.utils.data.Dataset` : 특정 데이터 한 개의 example를 return 해준다.
    - Dataset class : python obejct → Tensor
    - a single example 리턴.
    - 반드시 `__len__()` 함수, `__getitem__()` 함수를 구현해야 한다.
- `pl.LightningDataModule` : 데이터 자체를 관장하는 모듈
    - 데이터 자체를 전처리 하는 등.
    - 데이터를 병렬 GPU에 뿌려주는 역할까지 자동으로 다 해준다.
    - 내부에 데이터셋을 정의해야. : 한 개씩 return 하는 걸 여러 개 묶어서 미니배치로 처리 해준다.
- `pl.LightningModule` : 우리가 실제로 구현해야하는 뉴럴 네트워크의 본체.
    - pytorch의 `nn.Module`과 같다고 생각하면 된다. (nn.Module 구현하듯이 하면 됨)
    - trainer에 의해 자동으로 훈련되고 test됨.
- `pl.Trainer` : 트레이너 안에서 어떻게 훈련 시킬건지를 정하게 된다.
⇒ 위 **네 가지 Component를 구현**하면 하나의 NN 훈련기(예측기)가 만들어진다.

- `pl.seed_everything()`: 코드를 다른 사람이 돌렸을 때도 똑같은 결과가 나오도록 재현성 확보를 위해 만들어짐.
    - python, numpy, pytorch(, cuda)의 random seed를 설정해준다.
- argument setting(parser)
    - `parser = ArgumentParser()` 로 parser객체 생성.
    - `parser.add_argument()` 로 argument 추가.
    - `pl.Trainer.add_argparse_args(parser)`
    - 모델의 default 하이퍼 파라미터를 `model.add_model_specific_args(parser)` 로 자동으로 넣을 수 있음. 모델 specific한 arguments를 넣는 것.




### **datamodule, model, training, testing** 네 부분을 작성.
- datamodule
    - `prepare(), setup()` : 병렬 GPU 쓰는데에 많이 활용.
    - `__init__()` : train, test 데이터셋을 Dataset클래스 을 이용하여 만든다.
    - `torch.utils.data.random_split()` 을 이용하여 훈련 데이터셋과 검증 데이터셋 분리.
    - `*_dataloader()` : DataLoader(torch) 리턴.( * : train, val, test) ; 예약된 함수
        train_dataloader()는 대부분 shuffle = True를 인자로 넘겨줌. val, test는 하지 않음.


- model
    - `__init__()`
        - `self.save_hyperparameters()` : hyperparameter들을 다 저장할 수 있도록 한다.
            → ex ) 저장하면 `self.hparmas.learning_rate` 와 같은 식으로 쓸 수 있다.
        - 모델에 필요한 layer 등을 정의한다.
        - `forward()`: 모델의 feed forward를 정해줌.
    - `training_step(self, batch, batch_idx)` : 예약 함수. 훈련은 배치를 받도록 함. 훈련 관련된 feed forward, loss계산, loss의 back_propagation등의 내용을 넣어주면 된다.
        - batch의 형태는 Dataset의 `__getitem()__`의 리턴 형태를 보면 된다. 각각이 batch wise로 쌓여있음.
        - `self(image)` = `self.forward(image)`→ forward가 불림.
            → output : label_logits
        - output, label로 loss 계산. → `training_step()` 의 return 값이 loss.
        - `self.log()` : default로 텐서보드에 넣어줌. 파라미터는 어떤 이름으로 넣어줄 지 이름과 저장될 내용. 또 report되는 주기를 지정해줄 수 있고 어떻게 보일지 지정할 수 있음.
        → `training_step()` 함수가 호출되고 나서 parameter update가 된다.
        
    - `validation_step()` 은 한 에폭이 끝나고 호출. metric까지 계산되서 로그.(`self.log_dict()` )
    - `validation_step_end()` : 화면에 뿌려주도록.
    - `test_step()` → `trainer.test(model, test_dataloader = dm.test_dataloader())` 시 호출됨.
    - `configure_optimizers(self)` : optimizer와 lr 설정.


- `trainer = pl.Trainer()` 로 생성 후 `trainer.fit()` 으로 train, valid로 학습 진행.
    - `trainer = pl.Trainer()` : 여기에 파라미터로 `callbacks = [EarlyStopping(monitor=’val_loss’)]` 를 사용하면 overfitting이 날 것 같을 때 early stopping을 해준다.

- version number(몇 번째 학습인지)
- terminal cmd : tensorboard —-logdir* /lightning logs
    → 학습 Monitoring 가능.
    → ckpt 파일 : 모델 저장.
    


## Tensor Operation
- tensor dtype
    - numpy로 tensor를 만들면 dtype까지 모두 전달됨.
    - 텐서 생성 시 dtype을 지정 가능하다.
    - to() 를 이용하여 dtype을 바꿀 수 있다.
    - type()을 이용하여 dtype을 바꿀 수 있다.
- reshape and view
    - 메모리를 쓰는 게 다름.
    - `tensor.view()` : 데이터 자체의 메모리 구조를 건드리는 게 아닌 보는 view만 다르게 하는 것. → view로 생성된 객체는 view를 호출한 원래 객체와 같은 메모리를 가리키지만 shape만 바꿔서 보임. 따라서 하나의 데이터를 바꾸면 view로 생성한 객체도 바뀜.
- tensor device
    - torch는 device를 텐서 생성시 인자로 전달해 주어 지정 가능. 기본값은 CPU.
    - `tensor.to(device)`를 이용하여 바꿔줄 수도 있다.
    - GPU → CPU : `tensor.cpu()` 를 쓰면 CPU메모리로 내릴 수 있음.
    - `tensor.tolist()` or `tensor.numpy()` : CPU에 있는 tensor를 Python 리스트나 numpy로 변환.
- tensor
    - `tensor.item()` : tensor 값이 scalar인 경우 값을 가져온다. scalar가 아니면 에러 발생.
    - `tensor.chunk()` : 텐서 쪼개기. 쪼갠 텐서들의 튜플이 나온다.
    - `tensor.cat()` : 텐서를 concat 시킨다. concat 시킨 dim이 커진다.
    - `tensor.stack()` : 텐서를 쌓는다. 차원이 하나 더 늘어남.
    - `tensor.transpose()` : 텐서 전치.
    - `tensor.sqeeze()` : 1인 차원(empty dim)을 날린다. ↔ `tensor.unsqueeze()` : dummy dim 추가(1인 차원을 추가)
    - indices tensor를 이용하여 Indexing 가능. `x[:, indices]` → indices에 있는 column들을 인덱싱.
- Reduce Operation
    - `torch.max()` : dim을 명시할 수 있음. 텐서의 최댓값과 인덱스가 나옴.
    - `torch.argmax()` : 최댓값의 인덱스만 뽑음.