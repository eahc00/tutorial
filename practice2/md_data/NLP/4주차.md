  
## Transformer 101
  
- Seq2Seq(2014)
    - 여러 개의 정보를 하나의 숫자 덩어리로 바꾸는 기술 : 이게 해결 돼야 모든 Seq2Seq가 된다.
  
- **Blending**
    - 서로 다른 것들을 섞어서 기존에 없던 새로운 가치, 정보를 만들어내는 것
    - Sequence of information을 Blending 해보는 건 어떨까?
    - multiple item을 섞어내서 하나로 바꾸는 방법?

- transformer에서의 문제 의식
    - How to Encode Multiple Items?
    - How to encode Long-term Dependency?
    - How to Encode Sequential Information?
    
    공학적 질문
    - How to Encode fast?
    - How to make simple architecture for Encoder and Decoder?
→ Attention Is All You Need 논문(Transformer의 시작)
  
RNN → RNN + Attention —RNN 걷어내고 Attention만 쓰자 → Transformer


> [!info]
> - Transformer의 인코더만 떼놓은 것 → BERT(Encoder only model)
> - Transformer의 디코더만 떼놓은 것 → GPT(Decoder only model)


- Key Components of Transformer
    - **Attention**
    - **Masking**
    - **Scaling + Normalization + Residual Connection**
  
### Attention ; Dot-Product & QKV Pattern
[[Attention_Is_All_You_Need.pdf]]
  
ex) 용액 다섯개가 있는데 새로 개발된 용액 하나를 합쳤을 때 가장 크게 반응을 하는 용액 조합을 찾고 싶다.
→ 각각의 용액에다가 새로 개발된 용액을 다 넣어보고 반응을 지켜본다. 만약 두 개의 용액에서 각각 2, 6의 반응이 일어난다하면 2를 한번, 6을 세번 넣으면 되지 않을까? ⇒ Blended 용액
이때의 반응도가 Attention
  
위에서 새로 개발된 용액(Query Vector)
내가 가지고 있던 용액(Multiple Vectors)
→ 여기서 섞였을 때 가장 조합이 좋은 걸 찾고 싶은 것
⇒ Blended Vector
  
ex) vector x1, x2 ,x3의 중요도(이 중요도 : Attention이다.)를 구해서 변환시킴 → 그냥 더한다.(Elementwise Sum : Blending)
그럼 3개의 벡터가 들어가서 한 개 똑같은 shape의 벡터가 나오는 것이다.
  
9차원 벡터가 3개 있다고 하면? (v)
Attention을 해줄 수 있는 A라는 matrix(vector)가 있다고 가정. (A는 학습되는 것)
v * A = a(스칼라)
a들을 softmax 함수에다가 붙인다. → wa가 나옴.(확률분포) → 이렇게 나온 가중치 wa를 v에 다시 곱해서 변형시킨 벡터 v’를 만든다.
→ Element sum으로 Blending


#### Attention
- Attention with Context(Query)
    - v와 Q를 같이 고려한 a들이 나온다고 가정하고 위와 같은 과정을 진행.

##### Query-Key-Value pattern
- Attention : **Query-Key-Value pattern**
    - **Dot-product Approach** (General, Concat도 있으나 Dot만 보면 됨)
    - 가정 : Q와 X의 Shape이 같아야 한다.
    - 중요도 계산 시에 **Q와 X의 Dot product** 수행 → scalar 하나(=a)가 나온다.


- Query-Key-Value 패턴

  > [!note]
  > ![[CNU/3-1/자연어처리/images/Pasted image 20240429231532.png|250]]
  > - Q@K(@ : MatMul) = similarity($q, k_i$)  
  > - 이후 scale과 mask(optional)이라는 과정을 거쳐서 나온 Output에 softmax : $a_i$ = softmax($s_i$)
  > - $a_i$@V = attention
  
	- 정보를 idx로 변환(조그마한 정보로).
		Query를 받으면 Query도 조그마한 정보로 변환. → Key로 검색.
		DB는 Key-Value로 구성됨. Original Value로 인덱싱한 정보를 Key라고 한다.
	        
	- Attention Score ← Query와 Key의 similarity 비교.
		Score(Q, K) → a
		softmax(a) → attention score(wa)
		Attention ← wa * X
        

##### Dot-Product
- l1, l2 distance
    - **두 벡터간 각도를 이용하여 유사도** 계산. **cos similarity**
    - 두 벡터의 길이 * cos similarity = 두 벡터의 dot product
    - $u \cdot v$﻿ 는 cos similarity에 비례하므로 u, v가 고정이라고 생각하면 cos similarity가 결국 문제.
    - $cos \theta = \frac {u \cdot v} {||u||||v||}$﻿ $u = q, v = k_i$﻿가 된다.
    - 각각의 방향성을 가진 벡터(key들)와 query q와 방향(cos similarity)을 비교 → 제일 비슷한 방향성을 찾고 blending.
    - key들이 훈련되는 것. query와의 반응 key값들이 같은 방향을 가리키도록 학습되는것.
	- Query and Key have similar directions
		- → High similarity(from the dot-product)
		- → High possibility to survive in blending process


##### Scaled Dot-Product
> [!problem]
> 왜 **Scaled** Dot Product를 사용해야 하는가?
> - softmax에서 $e^x$﻿를 사용하는데 $x = u \cdot v$﻿이고 x가 커지면 $e^x$﻿가 너무 커지게 됨 → 모델이 터짐.(overflow or small gradient)
> - x가 음수면 변화폭이 별로 없으므로 0으로 죽어버린다.
>   
> - dot-product : scalar값의 summation인데, scalar값의 수는 vector의 dimension과 같다. scalar값은 범위가 없다. → 만약 벡터가 큰 dimension을 가진다면, dot product 값은 엄청 커지는 경향이 있다.


- **dim이 커져도 x가 너무 커지지 않도록 가공을 해야** → 이게 normalization
	- $\sqrt{d_k}$는 $q\cdot k_i$의 표준편차. → 정규화 시키는 것. 
	- $q \cdot k_i$의 $\mu$는 0
	![[CNU/3-1/자연어처리/images/Pasted image 20240429233004.png|500]]
	![[Untitled 41.png|Untitled 41.png|350]]
        

### Multi-Head Attention
> [!note]
> - single attention function보다 학습된 다양한 linear projection을 통해 Q, K, V를 $d_k$차원에 lienarly project하는 것이 유리함을 발견
> - 이렇게 project된 버전의 Q, K, V에 대해 attention 함수를 병렬로 수행한다. 이 값들은 다시 한번 projection 되어 최종 값이 된다.

- X(with Q, K, V)들을 여러 head를 지나게 함. head를 지나서 나온 벡터 각각의 하나로 concat시킴.
  
- X랑 Q, K, V의 관계?
	- X -WQ→ Q
	- X -WK→ K
	- X -WV→ V

- 이 W(가중치 행렬)들을 학습시키는 것.
- 이 **W가 여러 개**이면 → 여러 개의 가중치 행렬인 W를 사용하여 입력 X를 **다양한 관점에서 Q(query), K(key), V(value)로 매핑**한다. → **이게 multi head attention**
- Query가 자기 자신에게서 온다 → self attention
- 자기 자신(X)에서 query를 뽑아낸다.
- 하나의 head에 dependent가 걸려있는 파라미터들(W : 가중치 행렬)이 각각 존재.



#### Attention
- softmax해서 나온 **가중치 값이** X원본에 곱해지는 게 아니다 → **value에 곱해짐**. 이 값들을 **elementwise sum 시킨 값이 z.**
- attention score가 0 →상위 레이어에 올라가지 않음. 내부적으로 **selection의 기능.**
	→ **attention 계산하는 법 알아야**
	  (softmax((Q@k)/sqrt(d_k)) * V)
##### Step-by-Step illustration
- X * WQ = Q
- X * WK = K
- X * WV = V

- multihead마다 WQ, WK, WV가 따로 있음. → 헤드 개수만큼의 z가 나오고 이걸 concatenate한다. → 이 z들을 $W_O$ matrix를 가지고 z를 다시 한번 blending : projection ⇒ Z
    ![[CNU/3-1/자연어처리/images/Pasted image 20240430124208.png|300]]
      → 한 헤드당 하나의 z가 나옴. head 개수만큼의 z가 나온다. → 이걸 다 합쳐서($W_O$행렬과 행렬곱)해서 하나의 값으로 만든다.


### Add & Norm

- Normalization
	- 모양이 정규화 될 수록 학습이 잘 된다. 정규화 테크닉 필요
	
- **Layer Normalization**
	- Layerwise Normalization
	- ex) cv에서 (RGB : feature dim) 
		- RGB로 Normalization
		- 같은 차원에 있는 걸 norm
	-  ex2) 자연어에서는? 
		- token embedding dim을 받아서 그것들을 layer norm.
		- gamma term, beta term이 있다.
	
- **Batch Normalization**
	- Batchwise Nomalization
	- ex) cv에서 (RGB : feature dim) 
		- batch 방향으로 normalization. (R값, G값, B값으로 norm) 
		- 각 차원별로 같은 위치에 있는 것끼리 norm
	- ex2) 자연어에서는? 
		- pad심볼로 배치에서의 길이를 맞춰준다. 근데 Pad심볼과 진짜 token은 의미가 달라서 normalization에서 문제가 생긴다.

- residual Connection
	잔차를 더해주는 것. Residual Connection 확인.