
## Vector Database and Retrieval Augmentation Generation

- 현재 LLM을 실제로 쓸 수 있는 도구로 만드는 기술.

### Motivation
#### LLM and Hallucination
- LLM의 가장 큰 문제 : **환각(Hallucination)**
	- 사람 입장에서는 환각 증상이지만, 기계 입장에서는 환각이 아니다.
- 왜 일어나는가?
	- LLM의 기본적인 꼴은 **확률 모델**
		- A, B, C가 들어갔을 때 D가 나올 확률
		$$
P(D|A, B, C)
$$

	- transformer - decoder only 생성 모델에 대하여 이 모델을 훈련 시킬 때,
	- <\s>부터 → 단어 생성(A, B, C, ...) → <\/s>
		- P(A|<\s>)
		- P(B|<\s>, A)
	- 생성되는 **token만큼의 Object function**이 존재하는 것 (각 생성마다 loss 계산).
		- 100개 토큰이 들어오면 100개를 다 학습시키는 것.
		- Backpropagation은 이 모든 loss에 대해 한 번에 Backpropagation
	- Encoder only를 masking을 수행(보통 15%)하고 mask된 part에 대한 loss를 구한다. 
		- 100개 토큰이 들어오면 15개만 학습 시키는 것.
	- loss의 계산하는 sum 자체가 다르다.

> [!result]
> - 확률 모델은 과거에 학습한 데이터에 기반으로 대답할 수 밖에 없는 것.
> 	- 내부에 이게 논리적, 윤리적 등으로 맞는지 안맞는지에 대해 생각하는 메커니즘이 없다.
> 	- 그냥 확률 모델일 뿐.
> - 최대한 **가장 그럴듯한 단어의 나열을 만들어 내도록 학습**한 것이다.
> 	- A → B → C 이런 걸 배운거지 그 안에 내용을 전혀 고려하지 않는다.
> 	- 사람 입장에서는 말을 잘 만들어내지만 사실과는 무관한 환각처럼 보일 수 있지만, 모델 입장에서는 그냥 잘 하는 것.
> - 기술 입장에서는 문제가 없지만 상용 입장에서는 문제가 된다. → 위험, 불안.


- 왜 문제가 되는가?
	- Extractive
		- 기존에 사용 되던 방식.
		- ex) 검색기 - 검색기는 모두 문서에서 extractive 작업을 하여 indexing을 해놓고 쿼리가 들어왔을 때 extractive(인덱싱)된 것과 비교하여 결과를 낸다.
	- Abstractive
		- 사실인지 모른다. 가짜 정보를 퍼뜨릴 수 있다.
		- 학습된 데이터에 젠더, 인종 등의 문제가 있다면 모델도 이렇게 된다.
			- 데이터 기반이기 때문에.

#### LLM and Big Data
- 실무적인 문제도 있을 수 있다.
	- LLM의 기능성 입장에서 접근
	- LLM을 Agent, function으로 쓰겠다.
	- 내가 물어보고 싶은 **prompt가 너무 길다** → 안된다.
		- ex) 법률 책 등
		- transformer가 길어지는 것.
	- 왜 긴 Prompt를 받을 수 없는지 기술적인 장애?
		- transformer는 RNN을 대체하기 위해 나온 것.
		- RNN → Attention 매커니즘 → transformer
		- Attention에 문제가 있다.

	- RNN은 **linear하게 complexity(cost)가 증가**.
		- 문장이 길어지면 길어진만큼만 더 계산하면 된다.
		- 구조적으로 계산량은 적지만(이전 time step의 hidden state만 사용) long-term dependency 문제.
	- attention은 모든 걸 동시에 고려해서 한번에 블랜딩(global하게 본다).
		- long-term dependency 문제는 잡아주지만 Q-K-V의 개수가 N개가 된다. **$N^2$의 문제**를 매 turn마다 풀어야한다.
		- **N이 커질수록(문장이 길어질수록) 지수적으로 계산량**이 늘어난다.
	- 이를 해결하기 위해 나온 모델의 예 - mamba


#### LLM and Real-time data
- 고급 언어 모델의 핵심
	- 모델의 크기(parameter의 크기)
		- 데이터가 많고 훈련을 훨씬 많이한다는 것. 
		- 큰 모델의 모든 파라미터를 좋은 값으로 채워넣으려면 압도적으로 긴 시간과 압도적으로 많은 데이터를 넣어야
	- 내가 서비스하는 시점과 모델이 개발 된 시점 간에 gap이 존재한다.
		- LLM은 **최근의 정보**를 알지 못한다.
		- 이렇게 LLM이 알지 못하는 최근의 정보를 물어봤을 때 LLM의 환각 증상이 나타나도 문제, 안 나도 문제다.


#### LLM and Long-term Memory
- 사람은 오래 전 과거의 일에 대해서 다시 꺼냈을 때 어느 정도 떠올릴 수 있는 메커니즘이 있다.
- LLM은 어느 정도 오래된 prompt(memory)를 다 날린다.
- (아주 먼) **과거의 데이터일수록 처리하기 힘들 것**이다.


### Retrieval Augmented Generation(RAG)
![[CNU/3-1/자연어처리/images/Pasted image 20240613223141.png|500]]

- 검색 생성 강화
- LLM에다가 검색기를 붙여주겠다는 idea.
- 질문을 중개해주는 engine이 존재.
	- 바로 LLM으로 보내는 게 아닌 검색기에 먼저 보냄
	- **검색기에서 나온 검색 결과를 LLM에 같이 전달.**
	- **context**(검색 결과)가 주어지면 이걸 참고하여 답하게 된다.
		- 수능에 지문 같은 것.

- **추론의 대상과 언어 능력의 대상을 구분**시켜 주고 있다.
- LM → LLM에 차이
	- **추론 능력**이 들어간다.
	- LM : 언어 능력 - 한국어를 그럴듯한 한국어로 만들어 내는 능력(말을 잘 하는 능력).
	- LLM : 데이터를 섞어서 추론할 수 있는 능력까지 가는 것.
	- **추론의 대상이 context**가 되는 것.

- LLM에게 context를 바탕으로 추론하라는 힌트를 준것.
- Q-A과정에서는 말이 잘 되게 만들기만 하면 된다.


- context에 기반하여 대답을 하므로 **환각 증상이 완화**된다.
- 큰 데이터가 아니라 몇몇 문서만 집어 넣기 때문에 긴 prompt를 못 받는 문제도 해결할 수 있다.
- 실시간 데이터도 잘하면 해결 가능


#### Flow
![[CNU/3-1/자연어처리/images/Pasted image 20240613204340.png|500]]

- Big-Data(많은 양의 문서, 긴 문서) → Chunker
	- Chunker : 책 → page(분절화) → 문단, subtitle등 적당한 단위로 쪼갤 수 있다.
	- 이렇게 쪼개진 걸 chunk라고 할 수 있다.
- **Chunker**를 통해서 내가 가진 **문서를 쪼갠다.**
	- LLM이 받아들일 수 있는 범위보다 작게 만드는 것.
- 이 쪼개진 chuck(document)를 **Embedding API**로 넘긴다.
	- **문장을 하나의 벡터**로 만드는 게 Embedding.
	- sentence embedding, word embedding 등
	- Encoder only(BERT)
		- <\CLS>(처음)을 벡터화 시킨다.
		- <\CLS> : **classification의 약자**로 문장 전체의 정보를 요약한 벡터로 사용되기 위해 설계되었다. BERT는 학습 과정에서 <\CLS> 토큰을 통해 문장의 전체 의미를 학습한다.
	- Decoder only(GPT)
		- 문장 끝에 있는 벡터를 벡터화 하거나(옛날 방식)
			- 마지막 토큰의 임베딩은 문장 전체의 의미를 반영하는 벡터로 사용될 수 있다.
		- LLM2VEC을 사용
			- 모든 토큰의 임베딩 벡터를 평균낸다. 모든 토큰의 정보를 결합하여 문장의 전체적인 문맥을 반영한 벡터를 생성한다.
	- 이 Embedding API도 LLM일 것이다.
	- 근데 Embedding에 사용되는 LLM과 실제로 답변을 생성하는 LLM은 달라도 된다.

> [!etc]
> - openai text embedding
> 	- 내가 이미지를 주든, 텍스트를 주든 어떻게든 숫자 벡터로 바꿔준다.

![[CNU/3-1/자연어처리/images/Pasted image 20240612160609.png|400]]

- Document(chuck)에 대한 Embedding된 vector를 **pair-wise하게 관리(v - D)**
	- **Vector Database**
- 입력으로 들어온 Prompt를 Prompt Enhancer를 통해 **똑같은 Embedding API에 넘겨주어 같은 차원으로** 만든다.
- 이렇게 만든 Prompt Query와 가지고 있는 pair-wise한 쌍을 일대일 매칭을 시켜서 **가장 그럴듯한 문서**(원본, original text)를 뽑아온다. 
- → 이게 **RAG의 context**가 된다. 얘를 **앞쪽에 붙여서 LLM에 던져줌.**

> [!etc]
> - OS
> - database
> 	- DB
> 	- RDB(Relational DataBase) → SQL : DBMS(Oracle)
> 	- SQL, oracle, vectorDB에 익숙해져야
> - 통신

> [!info]
> - Vector space에서 얼마나 유사한가를 판단하는 방법
>	- 내적을 이용
>	- 거리를 이용
>	- ...
> - 이게 vector database에서의 DBMS에서 사용할 수 있다.

> [!etc]
> - LangChain
> - chromadb

> [!warning]
> - 그럼에도 불구하고 LLM은 확률 모델이므로 RAG를 완전히 믿을 수는 없다.
> - context 말고 다른 게 더 크게 반영될 수도 있다.


## Large Language Model 정리
### 생성형 AI
- LLM & Generative AI
- 표현학습(Representation Learning) + Self-Supeervised Learning + Large-Scale Data & Model


#### World Model
- **Foundation Model**
- LM → 할 수 있는 function이 어마어마하다.
- LM은 인터넷을 통해 학습
	- 텍스트, 이미지, 음성 등 여러 데이터가 다 가능..
- **Computational Model** : 계산 가능하고 수학적으로 해석 가능한 모델이 나옴


#### LLM - 어떻게 문제를 푸는가?
- Before LLM
	- Context → AI Engine → Answer
- After LLM
	- Context - Answer 문서 완성 형식
	- Generation whole passage

- Core LLM + task specific LLM
- 이걸 하기 위한 framework의 흐름은 알고 있어야 한다.

- 보안이 중요하면(국가기관, 대기업, ...)에서는 chatGPT를 쓸 수 없다.
	- inside로 가져와야
	- 성능이 보장돼야


### The Large Language & Vision Model will serve as the next operation system(OS)

- Application Stack → AI-enabled Application Stack

![[CNU/3-1/자연어처리/images/Pasted image 20240612112845.png|600]]

### Problem Definition & Solution
- 머지않아 AI가 인간보다 훨씬 더 문제를 잘 풀게될 것이다.
- 문제 풀이보다 문제 정의를 잘 하는 사람이 필요하다.
	- 문제를 던지는 사람
	- 가치 창출/기획
	- connecting dot
- 문제 풀이는 기계가 한다.


#### Application
![[CNU/3-1/자연어처리/images/Pasted image 20240613223224.png|500]]

- 이 중 각 step에서 몇 가지는 사용할 줄 알아야 한다.
- 한 application을 만들 정도는 돼야.







