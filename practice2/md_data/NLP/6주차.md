
# NLP Task Descriptions

> [!review]
>- seq2seq learning은 바라보는 frame이고 구현하는 기술은 transformer
>- 자연어 처리를 어떻게 바라봐야 하냐? -> seq2seq(N2N)

- Transformer로 어떻게 Benchmark Dataset Task를 풀어낼 수 있는지?

## Ax-b
### **Natural Language Understanding** : NLU
> [!question]
> 이걸 어떻게 구현할 거냐?  
> 이해하고 답변을 생성하는 생성파트

#### Approach 
- 자연어를 뜯어낸 다음에 분석하고 답변 생성
- 최근의 방식은 한 번에 이해하는 것

> [!Object]
> 뜯어내는 것까지 살펴보자

- NLU Evaluation


### Understading bia 'Entailment' Analysis
> [!definition]
> **Entailment** : 수반, 함의

- 세 가지로 문제를 해결하겠다.
	A라는 문장과 B라는 문장이 어떤 관계인지 찾아내는 것.
	1. 같은 뜻을 수반(**Entailment**)하고 있는 것
	2. **Contradiction** : 반대 되는 뜻을 가지는 것
	3. **Neutral** : 무관한 것.
→ 이 관계를 모두 알아낼 수 있는 기계가 있다면? 언어를 이해하고 있다면 이 관계를 Classification할 수 있을 것.


#### Data Design
- 두 개의 문장 쌍을 만들어서 관계 labeling을 한다.
- 모델에는 두 가지의 문장이 들어가고 Classification된다.
	- 대체로 **Entailment, Not Entailment**로 쓸 수 있다.

> [!info]
> BERT모델을 사용하여 Binary Classification 수행?  
> 입력 2개는 어떻게 사용할 건가? 입력을 A, B로 나눠서 진행.  
> \[SEP] 토큰으로 이어 붙이고 Segment를 분리한다.  
> -> Sentence1 \[SEP] Sentence2 -> Tranformer Model -> Label(Entailment or Not Entailment)


> [!note] 
> **GPT**  
> 문서의 앞쪽 부분을 잘라서 뒷 부분을 생성하도록 한다.   
> 이후 정답 문서와 비교  
> 이거를 인터넷에 존재하는 모든 문서로 수행.   
> -> 나올 법한 문서를 생성할 수 있다.

##### LLM를 사용하는 방법?
- prompt
```
Sentence1 : {Sentence1}
Sentence2 : {Sentence2}
" Do they have the same meaning? "
Options : Entailment, Not Entailment
```

-> 답변을 만들어 내는 것.
> [!important]
> 이런 식으로 이어지는 모든 Task에 LLM 적용 가능 : 유연하다  
> 옵션, 문장 등을 주고 질문함

> [!info]
> 성능은 Transformer가 현재는 더 좋지만 LLM은 계속 성능이 향상 중이다.



## BoolQ
### Questioning Answering
> [!summary]
> 질문에 대한 답이 'yes'인지 'no'인지 판단  
> question + passage

- Question + Passage(지문)
	- 지문과 질문을 줬을 때 답변을 Boolean으로 수행.
- **질문과 지문이 입력으로 들어가고 Boolean label이 나온다.**
- input : Question \[SEP] Passage -> Transformer Model -> **True or False** Lable
- 여기서 모델은 트랜스포머 모델의 Encoder일 수도, Decoder일 수도 있다.
-> Binary Classification이라 성능은 잘 나온다. (거의 90% 이상)


> [!tip]
> 모든 task에 다 적용할 수 있는 Engine을 만들어내야.


## CB(CommitmentBank)
### Sentiment Annotation
> [!summary]
> 텍스트가 다른 텍스트를 함축하는 지 판단  
> premise + hypothesis


- Persona(Model) : 사람이 가지고 있는 페르소나를 얘기한다. : Model의 페르소나가 만들어지는 것
- Dialogue : 사람과 모델이 대화한다.
	- 대화 중에 Entailment, Contradiction, Neutral을 구분
	- 모델의 대답 중 페르소나와 일치하는 것, 틀린 것, 무관한 것을 구분


#### Data Design
- Conversations -> Premise
- Document -> Hypothesis를 각각 추출
- **input : premise \[SEP] Hypothesis**

 - **Label은 Entailment, Contradiction, Neutral로 나옴.**

→ 성능은 거의 99로 나온다.


## CoNLL2003(Named Entity Recognition)
### Named Entity Recognition ; 개체명 분석
- 문서에서 핵심 정보만을 추출하고 싶다 -> 새로운 데이터베이스를 만들고 싶음


#### Data Design
- Document -> token 추출

- **input : Sentence**
- **label : NER Tags**

> [!definition]
> **개체명?**  
> 	날짜, 대전, 회사명 etc...  
> 	많이 활용되는 개체명 정리
> 1. Person : PER
> 2. Location : LOC
> 3. date : DAT
> 4. organization : ORG  
> 5. money  
> 
> 특히 1~4.을 사용함.

Begin-Money, Inside-Money... 라는 식으로 분류함. 
개체명이 아니면 O. (out)

- TAG
	IOB
	BIO
	- Inside
	- Out
	- Begin
	+) E를 추가하여 End를 표기할 수도 있음.
	+) S를 추가하여 한 글자 개체명을 표기할 수도 있음. (Single) 

1. 개체명인지 아닌지 BIO로 분류
2. 어떤 개체명인지 분류(B-개체명, I-개체명, O)

만약 개체가 (A, B, C) 있다면 각각 B, I가 존재하고 O는 하나 존재하므로 7개의 태그가 필요.
따라서 개체분류 **N개 -> N * 2 + 1의 태그 존재.**


#### Model architecture
- N2N 문제가 된다.
- input : Tokenized Sentence
- label : NER Tags
하나의 Token 당 N * 2 + 1개 class로 분류하는 문제가 된다.(N2N)
- 만약 맨 앞에 \[CLS]가 들어가면 \[CLS]에 대해서는 분류를 안하므로 실제로는 (N+1)2N이다.

##### LLM에서는 어떻게?
- Sentence를 주고 그냥 물어본다
- ex) 이 문장에서 person 뽑아봐. (person이 뭐냐?)
- 장점 : transformer에서 output은 숫자로 나오는 반면 LLM은 사람이 보기 편한 형태로 나온다.
- 단점 : 지 멋대로 해석해서 다른 이름을 줄 수도 있다. (ex-흥민손 -> 손흥민, M.J -> 마이클 조던)

> [!tip]
> spaCy : NLP 모델을 쉽게 갖다 쓸수 있음 -> 여러 task 가능


## COPA(Choice of Plausible Alternatives)
### Choose the cause or effect of a given sentence from two options

> [!definition]
> Plausible : 있을법한  
> 문제의 대안으로 가장 적절한 것을 찾아내는 것을 기대  
> 주어진 사실에 대해 가장 그럴듯한 원인 또는 결과를 선택

- **전제를 주고 CAUSE와 RESULT를 물어봐서 제일 적절한 대답을 고름.**

#### Data Design
- Documents -> premise 추출
- 뭐가 더 좋은 Alternative인지를 다 달아놓음.

#### Model architecture
- **input : Question, Premise, Choices**
	- input이 세 개인데 어떻게 해야할까?
	- 심지어 Choices도 여러 개임. 

	- Question \[SEP] Choice1
	- Question \[SEP] Choice2
	- 이렇게 만들어서 하나씩 Transformer Model에 집어넣어서 각각이 T/F인지 분류 (하나씩 학습시키는 것)

	- Question \[SEP] Choice1 -> transformer로 vectorA를 뽑아냄
	- Question \[SEP] Choice2 -> transformer로 vectorB를 뽑아냄
	- vertorA, vectorB를 다시 MLP나 간단한 모델에 넣어서 Binary Classification 되도록 한다.

- **Lable은 0 or 1이다.**

##### LLM에서는?
- 그냥 다 집어넣고 하나 골라보라 하면 됨.

성능도 거의 99 나옴.


## MultiRC
### Reading Comprehension : 독해
> [!summary]
> 여러 문장과 질문에 대한 복수의 답변을 평가  
> paragraph + question + answer

#### Data Design
- Document -> Passage 추출
- Answer도 만들어준다(T/F가 아닌 text로).

#### Model Architecture
- **input : Passage \[SEP] Question \[SEP] Answers** 
	- Segment Embedding도 넣어줌(0, 1, 2)
- **label : \[0 or 1, ...]**

이 정도 되면 성능도 잘 안나옴.
- 평가 지표 : F1(일부만 맞아도 점수를 줌), Exact Match(다 맞아야 줌)


## RTE(Recognizing Text Entailment)
> [!summary]
> 두 문장 간의 의미 함축 관계를 판단  
> promise + hypothesis


- **Premise + Hypothesis -> Lable(Entailment or not Entailment)**
- BoolQ랑 같음.


## WiC(Word in Context)
### Classify as "Same Meaning" or "Different Meaning"
- **Sentence 1, Sentence 2** (동음이의어?)를 주고 같은 의미인지 아닌지를 **T/F**로 분류.
- Sentence에 \[WORD] interested word \[\\WORD] 태그를 앞뒤로 달아서 관심 단어를 표기함.
	- ex) Do you want to com over to my  \[WORD] place \[\\WORD] last?


## WikiHow(Summarization Task)
### Summarization
- 생성 task.

#### Model Architecture
- **input : Passage**
- **output: Summarized passage**

##### LLM
 - Decoder만 있음
 - 인코더-디코더에서는 앞 부분 반을 주고 디코더에서 뒷 부분을 생성하게 하는데 그냥 전체를 요약으로 생성하게 한다.

##### 성능
- ROUGE : 30~50정도만 되면 괜찮은 것.
- BLEU

- 같은 의도를 표현하는 문장이 너무 많기때문에 정확하게 평가할 수는 없음.



## Another Task
> [!seealso]
> WSC(Winograd Scheme Challenge)  
> 문장에서 대명사의 참조 대상을 식별  
> text + span1_index + span2_index + span1_text + span2+text


> [!seealso]
> ReCoRD task  
> 뉴스 기사를 바탕으로 빈칸 채우기 문제를 해결  
> passage + query + entities + entitiy_spans + answer


> [!seealso]
> wikeann  
> 여러 국가의 언어로 Token 단위 Entity Tag로 구성된 데이터셋  
> tokens + ner_tags + langs + spans



## Benchmark
> [!seealso]
>NLP Benchmark는 모델의 성능을 객관적으로 평가하기 위해 여러 task들에서 시험할 수 있는 표준화된 데이터셋, 평가 메트릭, 그리고 평가 방법을 말한다.
>- **GLUE**
 >   - 문장 관계 추론, 감정 분석, 텍스트 유사성 등 다양한 task를 포함하는 언어 이해(NLU) 평가 benchmark  
>  
> - **SuperGLUE**
>    - GLUE보다 더 어려운 태스크(BoolQ, CB, COPA, etc…)를 포함하여, 언어 모델의 이해 능력을 더 깊게 평가하는 밴치마크.  
>
>
>- Large Language Model(LLM)
>   - 대규모 언어모델(LLM)의 언어 이해 및 문장 생성 능력을 평가하는 밴치마크로 BIG-bench 또는 Open LLM Leaderboard가 있다.

